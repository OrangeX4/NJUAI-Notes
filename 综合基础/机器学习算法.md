# 机器学习算法

## PCA

```python
def pca(X, k):
    # 1. Subtract the mean
    X = X - X.mean(axis = 0)
    # 2. Get the covariance matrix: 1/n * X^TX
    # cov_matrix = (1 / X.shape[0]) * (X.T @ X)
    cov_matrix = np.cov(X, rowvar=False)
    # 3. Calculate the eigenvalues and eigenvectors
    eigenvalues, eigenmatrix = np.linalg.eig(cov_matrix)
    # 4. Sort by eigenvalues and get matrix P (topK eigenvectors)
    idx = np.argsort(eigenvalues)[::-1]
    P = eigenmatrix[:, idx[:k]]
    # 5. Get X after dimensionality reduction
    X_pca = X @ P
    return X_pca
```

## LDA

```python
def lda(X, y, x):
    # w = S_w^{-1} * (m_1 - m_2)
    # 1. Calculate the mean of each class
    m1 = X[y == 0].mean(axis=0)
    m2 = X[y == 1].mean(axis=0)
    # 2. Calculate the within-class scatter matrix
    S_w = (X[y == 0] - m1).T @ (X[y == 0] - m1) + \
        (X[y == 1] - m2).T @ (X[y == 1] - m2)
    # 3. Calculate the weights
    w = np.linalg.inv(S_w) @ (m1 - m2).T
    # 4. Get the predicted label
    return 0 if abs(x @ w - m1 @ w) < abs(x @ w - m2 @ w) else 1
```


## KMeans

```python
def kmeans(X, k):
    # 1. Randomly choose k samples as initial centroids
    centers = X[:k]
    old_centers = np.zeros_like(centers)
    while not np.allclose(old_centers, centers):
        old_centers = centers.copy()
        # 2. Assign each sample to the nearest centroid
        dists = np.sqrt(((X - centers[:, np.newaxis]) ** 2).sum(axis=2))
        labels = np.argmin(dists, axis=0)
        # 3. Update centroids
        for i in range(k):
            centers[i] = X[labels == i].mean(axis=0)
    return centers, labels
```


## KNN

```python
def knn(X, y, k, x):
    # 1. Calculate the distance between x and each sample in X
    distances = np.sqrt(((X - x) ** 2).sum(axis=1))
    # 2. Sort by distance and get the topK nearest samples
    idx = np.argsort(distances)[:k]
    # 3. Get the labels of the topK nearest samples
    y_pred = y[idx]
    # 4. Get the most frequent label
    y_pred = np.bincount(y_pred).argmax()
    return y_pred
```


## Linear Regression

```python
def linear_regression(X, y):
    # 1. Add bias
    X = np.hstack([X, np.ones((len(X), 1))])
    # 2. Calculate weights
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w
```


## Logistic Regression

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def logistic_regression(X, y, lr=0.01, max_iter=1000):
    # 1. Add bias and initialize weights
    X = np.hstack([X, np.ones((len(X), 1))])
    w = np.zeros(X.shape[1])
    # 2. Iteration
    for _ in range(max_iter):
        # 3. Calculate the gradient
        y_pred = sigmoid(X @ w)
        grad = X.T @ (y_pred - y) / X.shape[0]
        # 4. Update the weights
        w -= lr * grad
    return w[:-1], w[-1]
```
