# 数据结构和算法

## 分析基础

1. 插入排序
    1. 类比整理扑克牌
    2. 不变量 (Invariant) 是 $A[1...j-1]$ 有序, 插入时从后往前找, 不适合的数字不断向后移一位, 直到找到合适的位置插入就终止内循环
    3. 最好情况下, 数组已经有序, 时间复杂度为 $O(n)$; 最坏情况下, 数组逆序, 时间复杂度为 $O(n^2)$
    4. 为了证明正确性, 我们需要证明对于任意输入, 算法均会终止, 并且给出正确的结果. 为了处理循环我们需要用数学归纳法证明, 其中会用到循环不变量
    5. 稍微修改就成为冒泡算法: 不断地将大的数字往后移, 循环不变量是 $A[j...n]$ 有序且是最大的 $n-j+1$ 个元素
2. 时间复杂度
    1. $f(n)\in O(g(n))$ 表示存在某些常量 $c$ 使得在 $n_0$ 之后 $f(n)\leqslant cg(n)$, 说人话, 就是一个可以达到的上界
    2. $f(n)\in \Omega(g(n))$ 表示存在某些常量 $c$ 使得在 $n_0$ 之后 $f(n)\geqslant  cg(n)$, 说人话, 就是一个可以达到的下界
    3. 既是 $O$ 也是 $\Omega$ 的话, 就成为 $\Theta$ 了, 即等阶
    4. $f(n)\in o(g(n))$ 表示对任何常量 $c$, 在 $n_0$ 之后 $f(n)< cg(n)$, 说人话, 就是一个达不到的上界, 和 $\Omega$ 恰好相反
    5. $f(n)\in \omega(g(n))$ 表示对任何常量 $c$, 在 $n_0$ 之后 $f(n)> cg(n)$, 说人话, 就是一个达不到的下界, 和 $O$ 恰好相反
3.  常用工具
     1.  洛必达法则, 用于比较两个函数的增长速度
     2.  斯特林近似: $\displaystyle  n!\sim  \sqrt{2\pi n}\left( \frac{n}{e} \right)^{n}, \sqrt{c_0 n}\left( \frac{n}{e} \right)^{n}\leqslant n!\leqslant \sqrt{c_1 n}\left( \frac{n}{e} \right)^{n}$


## 结构基础

1. 抽象数据类型 (Abstract Data Type): 一种接口, 描述的是这种数据结构 "可以做什么"
2. 队列 (Queue): `Add(x)` 和 `Remove()`
    1. FIFO 队列
        1. `Enqueue(x)`
        2. `Dequeue()`
    2. LIFO 队列, 栈 (Stack)
        1. `Add(x)` 或 `Push(x)`
        2. `Remove()` 或 `Pop()`
    3. 双端队列 (Deque)
    4. 列表 (List)
        1. `Size()`
        2. `Get(i)`
        3. `Set(i, x)`
        4. `Add(i, x)`
        5. `Remove(i)`
        6. 我们可以用循环数组来实现列表, 进而实现双端队列, 能达到 $O(1)$ 的时间复杂度
        7. 使用单向链表, 只要保存了 `head` 和 `tail`, 就适合用于实现 Stack 和 FIFO Queue
        8. 如果用双向链表实现的话, 记得加一个哨兵 (Sentinel), 或者说一个 `dummy` 节点, 可以避免 `head` 为 `null` 的情况
3. 应用
    1. 栈可以用来平衡符号 (Balancing Symbols)
    2. 栈可以用来计算后序表达式, 不需要括号和优先级, 就已经有能力表示和计算任何的二元表达式
        1. 设定两个栈, 操作数栈和操作符栈, 便可以计算中序表达式
    3. 两个 FIFO 可以实现一个 LIFO, 反之亦然
    4. 栈可以实现函数调用, 同理, 任何的递归也可以改用循环和栈来实现
    5. 尾递归可以直接改成循环, 不需要栈, 只需要在 `while(True)` 循环尾部将函数参数改为对应参数即可
    6. 随机队列: 添加时加入到数组尾部, 取出时随机取出, 然后将数组尾部元素移动到该位置
    7. 使用 $O(n)$ 的时间和 $O(1)$ 的空间可以逆转一个链表, 可以用递归的方式实现
    8. 使用异或可以只用一个指针就实现两个指针的效果


## 分治法

1. 分治: 将问题递归地划分为子问题, 分别进行处理, 最后将处理结果统合起来
2. 分治证明正确性: 数学归纳法, 奠基是基问题的结果, 归纳假设是子问题结果的正确性, 归纳步骤是合并子问题结果的正确性
3. MergeSort
    1. 进行二分, 然后在 $O(n)$ 的时间内将两个有序的数组融合 (Combine).
    2. 时间复杂度: $T(n)=2T(n/2)+O(n)$
    3. 每一层复杂度都是 $n$, 一共 $\log n$ 层, 最后的时间复杂度为 $T(n)=n\log n$
    4. 或者使用一个 FIFO 队列, 每次取出两个有序数组, 融合成一个有序数组之后重新放入队列中
4. 整数乘法
    1. $xy=x_{l}y_{l}\cdot 2^{n}+[(x_{l}+x_{r})(y_{l}+y_{r})-x_{l}y_{l}-x_{r}y_{r}]\cdot 2^{\frac{n}{2}}+x_{r}y_{r}$
    2. $T(n)=3T(n/2)+O(n)$
    3. $T(n)=O(n^{\lg 3})=O(n^{1.59})<O(n^{2})$
    4. 目前最快可达 $O(n \lg n)$
5. 矩阵乘法
    1. $T(n)=7T(n/2)+\Theta(n^{2})$
6. 替代法
    1. 已知 $T(n)=7T(n/2)+cn^{2}$
    2. 猜测 $T(n)\leqslant d_1n^{\lg 7}-d_2n^{2}$
    3. 用数学归纳法, 带入, 放缩即可
7. 递归树法
    1. 将递归树画出来, 分析每一层的消耗
    2. 指数递减: 只看第一层, 则 $T(n)=O(f(n))$
    3. 各层相等: 每一层加起来, 则 $T(n)=O(f(n)\lg n)$
    4. 指数递增: 只看最后一层, 则 $T(n)=O(n^{\log_{c}r})$
8. 主定理
    1. 即递归树法的增强版本, 对 $T(n) = a T(n / b) + f(n)$ 分析
    2. 通过分析 $f(n)$ 与 $aT(n/b)$ 的关系, 即可知道究竟是三种情况中的哪种情况
    3. 指数递增: $f(n)$ 比较小, $f(n)=O(n^{\log_{b}a-\epsilon})$, 则有 $T(n)=O(n^{\log_{b}a})$
    4. 各层相等: $f(n)$ 恰好相等, $f(n)=\Theta(n^{\log_{b}a})$, 则有 $T(n)=O(n^{\log_{b}a}\lg n)$
    5. 指数递减: $f(n)$ 比较大, $f(n)=\Omega(n^{\log_{b}a+\epsilon})$, 并且要求存在 $c < 1$ 与所有充分大的 $n$ 使得 $af(n/b)\leqslant cf(n)$, 则有 $T(n)=O(f(n))$
    6. 主定理并没有覆盖所有情况, 例如 $T(n)=2T(n/2)+n/\lg n$ 就无法使用主定理
9. 向上取整和向下取整一般不会影响结果, 子问题规模稍微加一点或者减一点一般也都不会影响最终的结果
10. 划分规模不等
    1. 先使用递归树法获得一个猜测值
    2. 再使用替代法证明结果是正确的
11. 使用 MergeSort 可以用来计算逆序对个数
    1. 逆序对 = 两个子序列的逆序对 + 融合过程中产生的新的逆序对
12. 一些奇怪的复杂度分析
     1. $T(n)=T(n-2)+T(n/2)+n$
         1. 画出递推树, 发现可以向左下角倾斜地计算, 每一层 $\displaystyle (\frac{n}{4^{i}})^{2}$ 最多出现 $n^{i}$ 次
         2. $\displaystyle T(n)\leqslant \sum_{i=1}^{\log n}(\frac{n}{4^{i}})^{2}\cdot n^{i}=n^{O(\log n)}$
     2. $T(n)=T(\alpha n)+T((1-\alpha)n)+cn$
         1. $T(n)=O(n\log n)$
13. 选举人问题
     1. 划分成两个子问题, 然后取出每一个组的主选举人和拥有相同派别的人数


## 堆

1. 二叉堆
    1. 最大堆
        1. 顶部有着最大值
        2. 每个子堆也是最大堆
        3. 是完全二叉树
2. 用数组表示二叉堆 (zero-based)
    1. 父节点: `(idx - 1) // 2`
    2. 左子节点: `2 * idx + 1`
    3. 右子节点: `2 * idx + 2`
3. 最大堆
    1. `HeapInsert()`
        1. 加到数组尾部, 然后与父节点比较, 大于则交换, 逐层向上
        2. $T(n)=O(\log n)$
    2. `HeapGetMax()`
        1. 返回数组的首值
        2. $T(n)=O(1)$
    3. `HeapExtractMax()`
        1. 取出数组头部
        2. 将数组尾部元素放到头部
        3. `MaxHeapify(0)`: 头部与两个子节点比较, 小于的话, 与更大的子节点交换, 逐层向下
        4. $T(n)=O(\log n)$
4. 优先队列
    1. 可以用最大堆实现优先队列
    2. `UpdatePriority(item, val)`: 更新优先级, 如果变大了, 则向上调整, 与 `HeapInsert` 类似; 如果变小了, 则向下调整, 与 `MaxHeapify` 类似. 但是如果你要获取到 `item` 对应的数组下标 `idx`, 则需要在建堆过程中维护一个 `id(item)` 到 `idx` 的哈希表
5. 堆排序
    1. 建好一个最大堆, 然后不断从中取出最大值
    2. 从数组中建堆
        1. 不断加入值
            1. $T(n)=n\log n$
        2. 或者直接用原数组建堆
            1. 逐层使用 `MaxHeapify(i)`
            2. 即 `for i in range(n / 2, 1, -1) do MaxHeapify(i)`
            3. $\displaystyle T(n)=\sum_{h=0}^{\log n}(\frac{n}{2^{h}}\cdot O(h))=O(n\cdot \sum_{h=0}^{\log n}\frac{h}{2^{h}})=O(n)$
    3. 不断取出最大值, 然后放入堆尾部
        1. $T(n)=n\log n$
6. 在最大堆中取出第 $k$ 大的元素
    1. 要求时间复杂度为 $O(k\log k)$, 而不是 $O(k\log n)$
    2. 使用另一个堆 $H$ 来协助, 先往 $H$ 里加入 $M$ 的最大值
    3. 不断在 $H$ 取出最大值, 然后往 $H$ 放入该最大值的两个子节点
    4. 执行 $k-1$ 之后, $H$ 里就是第 $k$ 大的元素了


## 排序问题

1. 排序算法的性质
    1. 插入排序
        1. $O(n^{2})$ 时间, $O(1)$ 额外空间
        2. 稳定
    2. 归并排序
        1. $O(n\log n)$ 时间, $O(n)$ 额外空间
        2. 稳定
    3. 堆排序
        1. $O(n\log n)$ 时间, $O(1)$ 额外空间
        2. 不稳定: 同值的相对序在建堆过程中可能会被打乱, 例如 `<2a, 2b, 1>` 堆排序的结果是 `<1, 2b, 2a>`
2. 选择排序
    1. 选出最小值, 通过 `swap` 放到数组头部
    2. 以此类推, 尾递归地进行, 也即可以转为循环
    3. $O(n^{2})$ 时间, $O(1)$ 额外空间
    4. 不稳定, 反例同样为 `<2a, 2b, 1>`
    5. 如果使用堆来实现, 就变成了变种的堆排序, 时间复杂度为 $O(n\log n)$, 但仍然是不稳定的
3. 冒泡排序
    1. 逐渐将最大的元素冒泡到数组尾部, 因此 `for i in range(n - 1, 0, -1)`
    2. 内循环为 `for j in range(i)`, 并且 `if A[j] > A[j + 1] then swap(A[j], A[j + 1])`
    3. $O(n^{2})$ 时间, $O(1)$ 额外空间
    4. 稳定
    5. 为了加速, 我们可以在内循环加入一个 `swapped` 来判断是否已经有序, 如果已经有序, 则可以直接退出, 如同插入排序一样, 数组本身有序的最优情况时间复杂度为 $O(n)$
    6. 甚至我们可以通过记录 `lastSwapIdx` 来令 `i = lastSwapIdx - 1` 而不是 `i = n - 1`, 这样可以拥有更快的速度
4. Shell 排序
    1. 例如排序 16 个整数
        1. 可以先将距离为 8 的元素分为一组进行组内排序
        2. 然后将距离为 4 的元素分为一组进行组内排序
        3. 然后将距离为 2 的元素分为一组进行组内排序
        4. 最后将距离为 1 的元素分为一组进行组内排序
        5. 得到有序数组
    2. 组内排序通常用插入排序
    3. 每一对 $i<j$ and $a_i>a_j$ 称为一个倒置 (inversion)
    4. Shell 排序前面的步骤是为了减少倒置的数量, 从而加速插入排序
    5. 我们可以通过选择不同的距离 `dist` 来改善 Shell 排序
5. 快速排序
    1. 具体步骤
        1. 选出主元 `x`
        2. 使用 `Partition` 将数组分为两部分 `B <= x` 和 `C > x`
        3. 递归地排序 `B` 和 `C`
        4. 输出 `<B, x, C>`
    2. 选择主元
        1. 目的是尽量让 `B` 和 `C` 的大小相近
        2. 任何的确定性算法都可以构造出最坏情况, 我们一般就使用最后一个元素作为主元
        3. 还可以加入随机化算法
    3. 划分 `Partition(A, p, r)`
        1. 维护两个指针 `i` 和 `j`, 其中 `i` 初始化为 `p - 1`, `j` 初始化为 `p`
        2. 我们认为 `B` 为 `[p, i]` 区间, `C` 为 `(i, j)` 区间
        3. 我们通过一个循环不断自增 `j`, 代表处理下一个元素
        4. 如果下一个元素 `A[j] <= x`, 则将 `i` 自增, 然后交换 `A[i]` 和 `A[j]`
        5. 最后交换 `A[i + 1]` 和 `A[r]`, 返回 `i + 1` 即可 (其中 `i + 1` 代表主元的位置)
        6. 可以注意到, 这种划分方式是不稳定的
    4. 最坏情况分析: 每次划分都达到 $0$ 和 $n - 1$ 的最坏划分, 时间复杂度为 $O(n^{2})$
    5. 平均情况分析: 用所有的 $n!$ 全排列的输入顺序取平均来分析
        1. 每次划分都很可能是合理的划分, 也即常数比例的划分
        2. 因此在两次好划分之间不会有太多的坏划分
        3. 坏划分的时间开销, 可以在不影响时间复杂度的情况下, 被好划分的时间开销所吸收
        4. 因此平均情况下的时间复杂度为 $O(n\log n)$
    6. 随机算法的时间开销
        1. 开销为 $O(n + X)$, 其中 $X$ 为划分时发生的比较次数
        2. 令随机变量 $X_{ij}$ 为 $A[i]$ 和 $A[j]$ 是否发生过比较, 易知 $X_{ij} \in \{0,1\}$, 且 $X_{ij} = 1$ 当且仅当 $A[i]$ 或 $A[j]$ 成为过主元
        3. 而想要 $A[i]$ 或 $A[j]$ 成为过主元, 则必须在 $A[i]$ 和 $A[j]$ 之间的某个元素成为过主元之前成为过主元
        4. 因此 $\operatorname{Pr}(X_{ij} = 1) = 2 / (j - i + 1)$
        5. 因此 $X = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\mathbb{E}(X_{ij}) = O(n \log n)$
    7. 其他问题
        1. 如果有许多重复元素, 只要我们只递归严格小于主元与严格大于主元的部分, 就可以将最坏情况转变为最好情况
        2. 不要在小数组上用快速排序
        3. 多主元也许会有帮助
6. 问题的上界和下界
    1. 对于某个具体的问题, 有其对应的上界和下界
    2. 对于某个具体的算法才有上界, 即最坏情况下算法 $A$ 所需要的时间
    3. 解决该问题的所有算法对应的上界, 取其最小值, 即是下界
7. 如何证明问题的下界
    1. 使用对手论证, 假设一个 Eve 在与你作对, 然后你问问题, 它回答, 你要问够足够多的信息才有可能正确回答这个问题
    2. 例如可以使用这种方式证明 $\Omega(n)$ 是排序问题的一个下界
8. 如果计算问题的下界
    1. 使用决策树法
    2. 我们使用比较的方法来进行排序, 每次比较都会分成两个分支, 所以是二叉树
    3. 排序问题的所有结果可能性, 共有 $n!$ 中, 即这棵树至少有 $n!$ 片叶子
    4. 这棵树的高度至少为 $\log n!=\Omega(n\log n)$
9.  桶排序
    1. 不基于比较, 类似于哈希表
    2. 可以直接使用 $d$ 个桶进行排序 ($k=1$): $\Theta(n + d)$
    3. 使用 $k$ 个桶, 每个桶内部 $d / k$ 个节点进行排序 (插入排序): $O(n + k + n^{2} / k) = O(n)$ 当 $k \approx n$
    4. 稳定
10. 基数排序
    1. 排序 $n$ 个 $d$-based 的数字
    2. `for (i=1 to d) use-a-stable-sort-to-sort-A-on-digit-i`
    3. 桶排序的一种应用场景, 因此时间复杂度可以达到 $O(dn)$
    4. 可以用来排序字符串


## 选择问题

1. 同时找出最大值和最小值
    1. 我们先两两分组, 然后找出 "局部" 最大值和最小值
    2. 然后从这些 "局部" 最大值和最小值中分别找 "全局" 最大值和最小值 
    3. 这也是我们能做到的最好了
        1. 给每个元素标号, "+" 表示可能为最大值, "-" 表示可能为最小值
        2. 我们要做的就是把标号清除
2. 通用选择问题
    1. 使用类似快排的做法
        1. `q = Partition(A)`
        2. `return A[q] if i == q`
        3. `return Select(A[1:q-1], i) if i < q`
        4. `return Select(A[q+1:n], i-q) if i > q`
    2. 我们可以证明, 预期时间复杂度是 $O(n)$, 最坏情况是 $O(n^{2})$
    3. 并且, 在找到之后, 会将数组左边变为都比其小的元素, 右边变为都比其大的元素, 这个性质在一些情况下会非常有用
    4. 我们可以使用 Median of medians 的方法, 将预期为 $O(n)$ 变为一定是 $O(n)$
        1. 将原数组分为 $n / 5$ 组, 每组包含 5 个元素
        2. 找到每一组的中位数, 组成拥有 $n / 5$ 个元素的新数组 $M$
        3. 找到 $M$ 的中位数 $m^{*}$ 作为主元
        4. 这样就有 $T(n) \le T(0.7n) + T(0.2n) + O(n)$, 这三项分别为左右子树最大的数据长度, 递归寻找 Median of $M$ 的开销, 以及计算 $M$ 与执行 `Partition` 的开销. 然后用替代法即可得 $T(n) = O(n)$.
    5. 我们可以知道此时的上界和下界是渐进一致的


## 树

1. 术语
    1. 子节点, 父节点, 叶节点, 兄弟节点, 祖先节点, 后代节点
    2. 深度 (depth): u 的深度是从 u 到 r 的路径的长度
    3. 高度 (height): u 到它所有的后代节点的路径中的最长路径长度
        1. 叶子节点的高度是 0
        2. $\displaystyle \operatorname{height}(u) = \max_{v \text{ is child of } u} \operatorname{height}(v) + 1$
    4. 因此有 $\operatorname{height}(r) = \operatorname{depth}(u)+ \operatorname{height}(u)$
2. 二叉树的分类
    1. 满二叉树: 任何一个节点要么没有子节点, 要么有两个子节点
    2. 完全二叉树: 类比堆, 其他层全满, 只有最后一层可以不满, 但是必须从左到右排列. 完全二叉树很适合用数组表示.
    3. 完美二叉树: 刚刚好形成一个三角形的那种情况, 即所有非叶节点均有两个子节点, 且所有叶节点均有相同的深度
3. 树的遍历
    1. 先序遍历: 先根节点, 后两个子树
    2. 后序遍历: 先两个子树, 后根节点
    3. 中序遍历: 从树的最左遍历到最右
    4. 层序遍历: 使用一个 FIFO 队列实现
4. 集合 (Set): 搜索, 插入, 删除
5. 有序集合 (OSet): 前序, 后继
6. 二叉搜索树
    1. 左子树的所有节点的 key 均小于根节点, 右子树的所有节点的 key 均大于根节点
    2. 中序遍历是升序遍历二叉搜索树
    3. 搜索可以递归地进行, 时间复杂度是树的高度
    4. 后继 (Successor): $x$ 的右子树中的最小元素, 或者是第一个左孩子也是 $x$ 祖先节点的 $x$ 祖先节点
    5. 插入: 就像搜索一样, 可以很简单地实现
    6. 删除:
        1. `z` 没有子节点, 直接删除
        2. `z` 有一个子节点, 用其子节点替换
        3. `z` 有两个子节点: 核心思想是用右子树的最小节点进行替换, 即 `BSTSuccessor(z.right)`
            1. `z.right.left == NULL`: `z = z.right`
            2. `z.right.left != NULL`: `y = BSTSuccessor(z.right), y.parent.left = y.right, z = y `
        4. 其中前三种情况都是 $O(1)$, 最后一种情况是 $O(n)$
    7. 选择: 左旋和右旋不改变 BST 的性质
    8. Treap: BST + Heap
        1. 分配给每个节点一个随机的优先级, 并且保证既要满足 BST 的性质, 也要根据优先级满足 Heap 的性质
        2. 注意: Treap 不一定是一个完全二叉树
        3. 为了构建 Treap, 我们可以先赋予每元素一个优先级, 然后按优先级排序后, 依次加入 BST 中
        4. 插入:
            1. 赋予随机优先级
            2. 依照 BST 性质插入 Treap 中
            3. 使用旋转将插入节点不断地 push-up 直到 Heap 性质满足
        5. 删除:
            1. 使用旋转将要删除地节点移到叶节点 (将较小的子节点旋转上来)
            2. 删除该叶节点
        6. 有着随机算法的优点和缺点
    9. 红黑树
        1. 性质
            1. 节点或红或黑
            2. 根节点是黑色
            3. 叶节点是黑色, 且均为 `NIL`
            4. `no-red-edge`: 不会有红节点相连
            5. `black-height`: 任何节点到它的任何叶节点的路径上均含有相同数目的黑节点, 定义 `bh(x)` 为不包含 `x` 自身的到叶节点路径上的黑节点数目
        2. 插入
            1. 标记 `z` 为红节点, 然后依照 BST 的性质插入
            2. 修复违反了红黑树性质的部分
    10. 跳表
        1. 插入: 抛硬币, 以 1/2 的概率不断地插入上一层链表


## 哈希

1. Direct-address Tables: 直接拿一个足够大的数组当表
2. 哈希函数: $h: U \to [m], m \ll |U|$
3. 链表式哈希表
    1. 搜索 `search` 用时 $O(k), k$ 是链表长度 (也即冲突程度).
    2. 插入 `insert` 用时 $O(1)$, 只需要插入链表头即可.
    3. 删除 `remove` 用时 $O(1)$ (存疑)
4. 简单均匀哈希假设
    1. 每个输入的 key 都被等可能地映射到每个 bucket, 且每个输入相互独立
    2. 加载因子 $\alpha=n/m$
    3. 平均搜索耗时 $O(1+\alpha)$
    4. 最坏情况预期耗时 ($m = Theta(n)$): $\Theta(\lg n / (\lg \lg n))$
5. 统一哈希
    1. 在构建哈希表的时候选择一个随机的哈希函数
    2. $\displaystyle P(h(x)=h(y))\leqslant \frac{1}{m}$ 对于所有 $x\neq y$
    3. 因此至多 $|\mathcal{H}|/m$ 哈希函数会导致 $h(x) = h(y)$
    4. 常见哈希族 $h_{ab} = (ak+b \mod p) \mod m$, 其中 $p$ 是一个大于最大可能键值的素数, `a in range(0, p), b in range(1, p)`
    5. 和简单均匀哈希假设区别在于不再依赖于输入的随机性
6. 开放寻址哈希
    1. 不再使用链表, 而是为每个元素探测一系列的 bucket 直到找到一个空的
    2. $h: U \times \{0, 1, \ldots, m - 1\} \to \{0, 1, \ldots, m - 1\}$
    3. 删除时只打上一个删除标记 `DEL`, 而不是将其删除成 `NIL`
    4. Probe
        1. Linear Probe: $h'(k) + i \mod m$ 导致 Clustering
        2. Quadratic Probe: $h'(k) + c_1 i + c_2 i^{2} \mod m$ 导致二级 Clustering, 以及同样的 $h'(k)$ 会导致同样的 Probe
        3. Double Hash: $h_1(k) + i h_2(k) \mod m$
        4. 三者均不满足统一哈希假设
        5. 不成功时 $\mathbb{E}[X] \le \frac{1}{1-\alpha}$, 成功时 $\mathbb{E}[X] \le \frac{1}{\alpha}\ln \frac{1}{1-\alpha}$
7. 完美哈希表
    1. 对于静态的 keys, 可以实现最坏情况 $O(1)$, 空间开销有限的 OSet
    2. 空间为 $m=n^{2}$ 即 $O(n^{2})$ 的哈希表只需要随机选取哈希函数即可
    3. 空间为 $O(n)$ 的完美哈希表
        1. 先建一个空间为 $O(n)$ 的一级链表式哈希表
        2. 对于每个含有多个元素的链表, 建造 $O(n^{2})$ 的二级完美哈希表
        3. 组合起来就是一个空间为 $O(n)$ 的哈希表了
        4. 但是只支持查询操作


## 均摊开销

1. 均摊开销
    1. $c_i$: 第 $i$ 次操作的实际开销
    2. $\hat{c}_i$: 第 $i$ 次操作的均摊开销
    3. 满足 $\sum_{i=1}^{k} c_i \le \sum_{i=1}^{k} \hat{c}_i$ 对于任意 $k \in \mathbb{N}^{+}$
2. 会计法
    1. 借助 "存款" 与 "取款" 的想法
    2. $B = \sum_{i=1}^{k} (\hat{c}_i - c_i) \ge 0$
    3. 然后用数学归纳法证明
3. 势函数法
    1. 势函数也像是 "余额"
    2. 将当前状态 $D_k$ 映射到一个数值 $\Phi(D_k)$, 因此一般会比基于状态序列的会计法方便简单
    3. 通常 $\Phi(D_0) = 0$
    4. 要求 $\Phi(D_k) \ge \Phi(D_0)$
    5. $\hat{c}_i = c_i + \Phi(D_i) - \Phi(D_{i-1})$


## 并查集

1. 并查集接口
    1. `MakeSet(x)`
    2. `Union(x, y)`
    3. `Find(x)`
2. 并查集可以用来实现连通图的判定
3. 基于链表的并查集
    1. 使用最普通的方式的话, 一系列 `Union` 操作的时间复杂度为 $O(n^{2})$
    2. 基于权重的话, 一系列 `Union` 操作的时间复杂度为 $O(n\log n)$
4. 基于树的并查集
    1. 基于路径压缩, 每次 `Find` 的时候将路径上的节点的父节点都改为根节点
    2. 分析时间复杂度的话, 改为 `PartialFind`, 时间复杂度接近于 $O(1)$
    3. `Find(x)`: `parent(x) = Find(parent(x)) if x !== parent(x)`
    4. `Union` 可以直接使用 `Find(x)` 与 `Find(y)` 来实现
    5. 只要初始化一个 `n` 并且每次 `Union` 的时候进行 `n--` 即可


## 图

1. 图的表示方法
    1. 邻接矩阵
        1. 节点数 $|V| = n$, 边数 $|E| = m$
        2. 无向图邻接矩阵是对称矩阵
        3. 对角线上的边要存在自环时才有值
        4. $A^{2}$ 代表着节点间两步长路径数
        5. 空间开销总为 $O(n^{2})$
        6. 易于判断两个顶点是否相邻, 难于穷举所有边
    2. 邻接链表
        1. 无向图的边会出现两次
        2. 空间开销小, 为 $O(n + m)$
        3. 易于穷举所有边, 难于判断两个顶点是否相邻
2. 图遍历
    1. 宽度优先搜索 (BFS)
        ```python
        from collections import deque

        def sssp_by_bfs(graph, s, t=None):
            visited = {s}
            dist = {s: 0}
            que = deque()
            que.append(s)
            while len(que) > 0:
                u = que.pop()
                for v in graph[u]:
                    if v == t:
                        return dist[u] + 1
                    if v not in visited:
                        que.append(v)
                        dist[v] = dist[u] + 1
                        visited.add(v)
            return dist
        ```
        1. 正确性: 使用归纳法证明, 证明在任意 $d$ 步时刻都有 `dist` 是正确的
        2. 引理: 可以通过 BFS 得到最小生成树
        3. 多个连通分量: 使用同一个 `visited` 然后对所有节点各尝试进行一遍 BFS 即可
    2. 深度优先搜索 (DFS)
        1. 树边: DFS 森林里的边
        2. 回边: 连接节点和其祖先节点的边
        3. 向前边: 连接节点和其孙代及往后节点的边
        4. 交叉边: 其他类型的边
        5. 括号定理: 发现时间和完成时间就像匹配的左右括号一样嵌套
        6. 白路径定理: $v$ 为 $u$ 的后代节点当且仅当 $u$ 被发现时 $u$ 和 $v$ 之间有一条全为白色顶点的路径
        7. 无向图的边要么是树边要么是回边
            1. 可以通过 DFS 运行时节点 $v$ 的颜色来判断边类型
            2. 对于无向图来说, 只可能是 "灰 -> 白" 和 "灰 <- 灰" 两种情况
            3. 即分别为树边和回边
3. 宽度优先搜索可以用来判断无向图的二部图
    1. 没有奇数环的无向连通图的二部图
    2. 给每个节点附上 BFS 生成的 `dist` 值, 即可通过其判断
4. 深度优先搜索的应用
    1. 有向无环图 (DAG)
        1. DAG 可以用于建模因果关系, 层次结构和依赖关系
        2. 可以用 DFS 判断是否存在回路: DAG 在 DFS 中不会出现回边
            1. 可以使用反证法证明
    2. 拓扑排序
        1. 每个 DAG 都有其拓扑排序
        2. 每个 DAG 至少有一个源点和一个聚点 (也就是说可以有多个)
        3. 算法: 从任意节点开始, 对所有节点进行进行 DFS, 计算所有节点的完成时间, 当节点完成时加入链表头部
        4. 同样的, 还有一种办法是不断从 DAG 中取出源点
    3. 强连通分量 (SCC)
        1. 强连通分量指有向图中的满足点互相连通的极大子图
        2. 有向图的强连通分量图 (Component Graph) 是 DAG
        3. 算法:
            1. 计算 $G^{R}$
            2. 在 $G^{R}$ 中运行 DFS, 计算完成时间 $f$
            3. 依照 $f$ 的逆序来进行 `DFSAll`
            4. 每一棵 DFS 树就是一个 SCC
            5. 时间复杂度 $O(|V|+|E|)$
        4. Tarjan 算法
            1. 从任一节点开始 DFS
            2. 如果我们能依次辨认出聚点 SCC 的根节点, 就能知道 SCC 了
            3. `low(u)` 是从 `u` 出发, 经过至多一条非树边, 能够到达 $C_u$ 中所有节点中发现时间最小的节点
5. 最小生成树 (MST)
    1. 简单想法: 不断地往 $A$ 里面加入 "安全边", 直到边数等于 $n - 1$
    2. 切属性 (Cut Property)
        1. 切 (Cut) 是 $G = (V, E)$ 的一个划分 $(S, V - S)$
        2. 若一条边的两个端点分别位于 $S$ 和 $V - S$, 则称该边跨越了切 $(S, V - S)$
        3. 若 $A$ 中没有边跨越切，则称切遵循 (respect) 边集 $A$
        4. 轻边: 跨越切的边中权重最小的边
        5. 切属性: 假设 $A$ 包含于 MST 中的边集, 对任意遵循 $A$ 的切 $(S, V - S)$ 来说, 若 $(u, v)$ 是轻边, 则 $(u, v)$ 对 $A$ 来说是安全的
    3. Kruskal 算法
        1. 由切属性可知, 最小权重外边 (连接两个 CC 的边) 对 $A$ 来说是安全的
        2. 将边按照权重排序 (时间复杂度为 $O(|E|\log |E|)$)
        3. 为每个顶点设立一个并查集
        4. 按照边权重升序遍历每一条边, 通过并查集判断他们是否成环 (即边两端是否同属一个 CC)
            1. 不成环就将边加入 $A$
            2. 并且将两个顶点 `Union`
        5. 时间复杂度为 $O(|E|\log |V|)$
    4. Prim 算法
        1. 从任意一个顶点开始, 设定 `dist`, 然后为所有顶点建立一个优先队列
        2. 进入循环, 不断从优先队列取出顶点, 并设置 `u.in = True`
        3. 对与 `u` 相连的每个顶点 (边) 进行遍历, 并在优先队列中更新
        4. PS: 此处与迪杰斯特拉算法的不同在于, 是更新 CC 与其他顶点的距离, 而不是 `s` 与其他顶点的距离
        5. 时间复杂度为 $O(|E|\log |V|)$
    5. Boruvka 算法
        1. 并行化的 Prim 算法, 同时对所有连通分量进行合并
    6. 无向图边权重不同, 则最小生成树不同
6. 单源点最短路径 (SSSP)
    1. 四种情况:
        1. 单位权重: BFS
        2. 任意正数权重: Dijkstra
        3. 任意权重无环: DAGSSSP
        4. 任意权重: Bellman-Ford
    2. Dijkstra 算法
        ```python
        for u in V:
            u.dist = float('inf'), u.parent = None
        s.dist = 0
        que = priority_queue(V, dist)
        while len(que) > 0:
            u = que.extract_min()
            for v in graph[u]:
                if v.dist > u.dist + w(u, v):
                    v.dist = u.dist + w(u, v)
                    v.parent = u
                    que.update(v, v.dist)
        ```
        1. 从任意一个顶点开始, 设定 `dist`, 然后为所有顶点建立一个优先队列
        2. 进入循环, 不断从优先队列取出顶点
        3. 对与 `u` 相连的每个顶点 (边) 进行遍历, 并在优先队列中更新
        4. 时间复杂度为 $O((|V| + |E|)\log |V|)$
    3. Bellman-Ford 算法
        ```python
        for u in V:
            u.dist = float('inf'), u.parent = None
        s.dist = 0
        for _ in range(n - 1):
            for u, v in edges:
                if v.dist > u.dist + w(u, v):
                    v.dist = u.dist + w(u, v)
                    v.parent = u
        for u, v in edges:
            if v.dist > u.dist + w(u, v):
                return 'Negative cycle'
        ```
        1. 基于对最短路径上跑 Dijkstra 中的 update 肯定不会出事的思想
        2. 我们进行 n - 1 次循环
        3. 每次循环对每一条边进行处理, 更新 dist
        4. 时间复杂度为 $O(|E|\cdot |V|)$
        5. 并且如果我们加上第 n 次循环, 就可以判断是否有负环
    4. DAGSSSP 算法
        ```python
        for u in V:
            u.dist = float('inf'), u.parent = None
        s.dist = 0
        topo_order = dfs(graph)
        for u in topo_order:
            for v in graph[u]:
                if v.dist > u.dist + w(u, v):
                    v.dist = u.dist + w(u, v)
                    v.parent = u
        ```
        1. 对于有向无环图, 我们只要使用 DFS 进行一次拓扑排序, 就能知道所有最短路径均位于拓扑排序中
        2. 按照拓扑排序进行 update
        3. 时间复杂度为 $O(|E|+|V|)$, 甚至小于 Dijkstra 算法
        4. 应用上: 可以用来判断关键路径, 可以说是迭代式动态规划在图论上的一个算法
    5. A* 算法
        ```python
        for u in V:
            u.est_to_s = inf
            u.est_to_t = h(u, t)
            u.metric = u.est_to_s + u.est_to_t
        s.est_to_s = 0
        s.metric = s.est_to_s + s.est_to_t
        que = priority_queue(V, metric)
        while len(que) > 0:
            u = que.extract_min()
            for v in graph[u]:
                if v not in que or v.est_to_s > u.est_to_s + dist(u, v):
                v.est_to_s = u.est_to_s + dist(u, v)
                v.metric = v.est_to_s + v.est_to_t
                v.parent = u
                que.add_or_update(v, v.metric)
        ```
        1. 使用启发函数 `h(u, t)` 来估计 `u` 到 `t` 的距离 (需要高估)
        2. 和 Dijkstra 算法几乎一致, 区别在于优先队列使用的权重不同, 以及是否需要重新将 `v` 加入到队列中
        3. Dijkstra 使用的是 `u.dist` 或者说就是 `u.est_to_s`, 因此也不可能需要重新将 `v` 加入到队列中
        4. 而 A* 算法有可能将 `v` 重新加入到队列中
7. 全配对最短路径 (APSP)
    1. Johnson 算法
        1. 修改 Dijkstra 算法以适应负边
        2. 只需要 $\hat{w}(u,v)=h(u)+w(u,v)-h(v)\geqslant 0$
        3. 我们加入一个节点 $z$, 其与所有 $G$ 中的顶点以 $0$ 权重的边连接
        4. 则只需要 $\hat{w}(u,v)=\mathrm{dist}(z,u)+w(u,v)-\mathrm{dist}(z,v)$
        5. 算法:
            1. 创建包含 $z$ 的新图
            2. 通过 Bellman-Fold 算法获取 $h(v)=\mathrm{dist}(z,v)$
            3. 基于 $\hat{w}(u,v)=h(u)+w(u,v)-h(v)$ 进行 Dijkstra 算法
        6. 时间复杂度为 $O(|V|^{3}\log|V|)$
    2. Floyd-Warshall 算法
        ```python
        for u, v in itertools.product(V, V):
            if (u, v) in E:
                dist[u, v, 0] = w(u, v)
            else:
                dist[u, v, 0] = math.inf
        for r in range(1, n + 1):
            for u in V:
                for v in V:
                    dist[u, v, r] = min(dist[u, v, r - 1],
                        dist[u, x[r], r - 1] + dist[x[r], v, r - 1])
        ```
        1. 给节点定义任意顺序 $x_1, x_2, \ldots, x_n$
        2. 定义 $V_r = \{ x_1, x_2, \ldots, x_n \}$
        3. 定义 $\operatorname{dist}(u, v, r)$ 为仅使用 $V_r$ 中节点作为路径 **内部节点** 的的最短路径
        4. $\mathrm{dist}(u,v,r)=\begin{cases} w(u, v), \quad r = 0 \land (u, v) \in E \\ \infty, \quad  r = 0 \land (u, v) \notin E \\ \min\begin{pmatrix} \mathrm{dist}(u,v,r-1), \\ \mathrm{dist}(u, x_{r},r-1)+\mathrm{dist}(x_{r}, v,r-1) \end{pmatrix}, & \text{otherwise} \end{cases}$
        5. 先设置每条边 $(u,v)$ 的 dist 即 `dist[u, v, 0] = w(u, v)`, 否则正无穷
        6. 迭代 $n$ 次, 并对每个顶点对 $u, v$ 进行更新, 依照上面提到的那个数学表达式
        7. 时间复杂度为 $O(|V|^{3})$



## 贪心与动态规划

1. 贪心算法
    1. 活动选择问题
        1. 算法
            1. 按照结束时间进行排列
            2. 总是选取结束最早且兼容的活动加入
        2. 证明
            1. 先证明贪心策略, 最早结束的活动必然位于一个最优解法中, 可以使用反证法证明
            2. 然后证明最优子结构性质, 即可以分解成最早结束的活动和剩下活动对应的子问题
            3. 然后使用数学归纳法证明
    2. 最优子结构: 问题的最优解法包含其子问题的最优解法
    3. 贪心策略: 不用知道子问题的最优解法的结果, 便能做出当前问题的选择
    4. 拥有最优子结构是贪心算法和动态规划算法的前提, 是否拥有贪心策略是二者的差别
    5. 可分背包问题
        1. 物品可分
        2. 计算所有物品的利润比, 进行排序, 从利润高到低不断选取
    6. 零一背包问题
        1. 物品不可分
        2. 不能使用贪心算法
        3. 可以使用动态规划算法
    7. 霍夫曼编码
        1. 将所有字符的频次统计出来
        2. 将其变为一棵满二叉树
        3. 不断合并两个最低频次的字符
        4. 使用优先队列来实现
        5. 复杂度 $O(n\log n)$
    8. 集合覆盖
        1. 总是选取当前能覆盖最多剩余节点的集合
        2. 但是这只能给出接近最优的解法
        3. 设 $k$ 为最优使用集合数, 则能保证最多只用 $k\log n$ 个集合
            1. 设 $n_{t}$ 是经过第 $t$ 次迭代后的剩余节点数
            2. 由最优解法是 $k$ 可知 $n_{t}$ 能被 $k$ 个集合覆盖
            3. 所以每一次迭代至少能覆盖掉 $n_{t} / k$ 个剩余节点
            4. 因此 $n_{t}\leqslant n_{t-1} - n_{t-1}/k\leqslant n_{0}(1-1/k)^{t}<n_{0}(e^{-1/k})^{t}=ne^{-t/k}$
            5. 因此 $t=k\ln n$ 便有 $n_{t}<1$, 即终止
2. 动态规划
    1. 木棒切割问题
        1. 最优子结构: $\displaystyle r_{n}=\max_{1\leqslant i\leqslant n}(p_{i}+r_{n-i})$
        2. 但是不能使用贪心策略, 可以找出反例
    2. 动态规划可以有两种形式
        1. 递归型动态规划
            1. 自顶向下算法
            2. 使用一个数组或哈希表进行对子问题答案的记录
            3. 易于理解, 但是代码量和空间复杂度相对较大
        2. 迭代型动态规划
            1. 自底向上算法
            2. 使用一个数组对子问题答案进行记录
            3. 找出一个顺序, 使得能够保证当前问题的所有子问题已解决
                1. 一维一般就是从开始到结尾或从结尾到开始
                2. 二维一般就是从矩阵左上角方块向右下角扩展
                3. 图论一般就是拓扑排序
    3. 矩阵乘法
        1. 可以使用结合律减小计算开销
        2. 不管是什么顺序, 最后步骤一定为 $(A_1A_2\cdots A_{k})\cdot (A_{k+1}\cdots A_{n})$
        3. 所以有 $\displaystyle m[i,j]=\min_{i\leqslant k<j}(m[i,k]+m[k+1,j]+p_{i-1}p_{k}p_{j})$
    4. 编辑距离
        1. 判断两个字符串的相似程度
        2. 对字符串最末尾分析, 可以分成三种情况
            1. $\begin{matrix} - \\ B[n] \end{matrix}$ 或 $\begin{matrix} A[m] \\ B[n] \end{matrix}$ 或 $\begin{matrix} A[m] \\ - \end{matrix}$
            2. 然后就可以拆分成子问题, 并有数学表达式
            3. $\mathrm{dist}(i,j)=\min(\mathrm{dist}(i,j-1)+1, \mathrm{dist}(i-1,j)+1, \mathrm{dist}(i-1,j-1)+I(A[i] \neq B[j]))$
    5. 最大独立集
        1. 独立集是从图中选出一系列不相邻顶点的集合
        2. 如果有环就很难判断, 但是对于树很好分析
    6. 最短路径问题有最优子结构性质, 最长路径问题没有最优子结构性质
    7. 自底向上的迭代型动态规划, 在一定情况下可以减小空间开销
        1. 例如 Floyd-Warshall 算法, 可以将 `dist[u,v,r]` 变为 `dist[u,v]`, 效果一致, 空间开销却从 $O(n^{3})$ 降为 $O(n^{2})$
        2. 编辑距离问题这种二维问题, 也可以通过保存一个 `distLast[n]` 和 `distCur[n]` 来替换原来的 `dist[n, r]`, 空间开销从 $O(n^{2})$ 降为 $O(n)$
    8. 子集求和问题, 无法用动态规划减小时间开销
        1. 给定元素个数为 $n$ 的整数集合, 是否有一个子集相加等于 $T$
        2. 用最简单的遍历法, 时间开销为 $O(2^{n})$
        3. 用动态规划
            1. 定义 $ss(i,t)=true$ 当且仅当 $X[i\cdots n], t$ 有解
            2. $ss(i,t)=\begin{cases} true, & t=0 \\ ss(i+1,t), & t<X[i] \\ false, & i > n \\ ss(i+1,t)\lor ss(i+1,t-X[i]) \end{cases}$
            3. 时间开销为 $O(nT)$



## 计算理论

1. 图灵机
    1. 无限纸带
    2. 可以左右移动, 以及在纸带上读写符号的头
    3. 存储了有限多的状态中的当前状态的寄存器
    4. 一个有限的指令表, 可以根据当前状态和当前读取符号映射到动作
2. 判定问题: 预期回答为是或否的问题
3. 优化问题: 在所有可行解中寻找最优化目标的一个解
4. 判定问题和优化问题可以互相转换
    1. 例如对于一个优化问题, 我们可以用二份或者枚举的方式判断一个解是否是最优解, 这就将优化问题转成了多次判定问题
5. 图灵机解决一个判定问题
    1. 图灵机对问题的每一个样例, 在有限步内, 都能正确地输出结果, 并且停止
6. 停机问题: 能否判断一个程序是否会终止
    1. 没有图灵机可以解决停机问题
7. P 类问题
    1. 对于判定问题集合 $\mathcal{P}$ 来说, 令 $I$ 为 $\mathcal{P}$ 的一个实例
    2. $|I|$ 是 $I$ 的二进制编码长度
    3. $\mathcal{P}$ 的算法 $\mathcal{A}$ 有着多项式上界, 仅当 $\mathcal{A}$ 运行时间对所有 $I$ 来说均为 $(|I|)^{O(1)}$.
    4. P 类的问题被称为容易解决的问题
    5. P 类问题有闭包性质: $T_{A}(n) = O(n^{2}), T_{B}(n) = O(n^{10}), T_{B \circ A} = O(n^{20})$
8. 非确定性图灵机
    1. 在碰到分支的时候, 可以同时执行很多动作, 也即同时有很多运行
    2. NTM 返回是, 当且仅当有某些运行停止且返回是
9. NP 类问题
    1. 可以有非确定性图灵机在多项式时间内判断的判定问题集合
    2. NP 意味着 "非确定性多项式时间"
    3. NP 类算法可以抽象为: 随机猜一个答案, 并交由一个分支图灵机来判断
    4. 因此 $P \subset NP$, 但是我们仍然不知道是否有 $P \neq NP$
10. 在可判定问题集合中, 还包含着其他非 NP 问题集合


