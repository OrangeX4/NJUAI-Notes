# 高级机器学习

总评: 30% 平时 + 70% 期末
试卷构成
    - 选择题: 10 题 * 2 分
    - 判断题: 5 题 * 2 分
    - 问答题: 2 题 * 15 分 + 4 题 * 10 分
    - 附加分: 10 分
考试方式: A4 纸开卷

**特征选择**: 从给定的特征集合中选择出相关特征子集的过程. 特征选择是一个重要的数据预处理过程. 特征选择的原因, 第一是减少维数灾难问题. 第二是去除不相关特征往往会降低学习任务的难度. **特征选择与降维**: 特征选择主要是去除一些无关特征和冗余特征, 并没有改变选择出来的特征. 降维是将特征映射到了一个新的低维空间中. 有一些算法可以同时进行特征选择和降维, 如 sparse PCA. **特征子集搜索与评价**: 根据评估结果获取下一个候选特征子集称为特征子集搜索. 逐渐增加相关特征的策略称为前向搜索. 逐渐减少相关特征的策略称为后向搜索. 将前向和后向搜索结合起来的策略称为双向搜索. 评价特征子集好坏的方法称为特征子集评价. 常见的有信息增益, 属性子集为 $A$, $V=v^{|A|}$, $\operatorname{Gain}(A)=\operatorname{Ent}(D)-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}\operatorname{Ent}(D^{v})$, 其中信息熵 $\operatorname{Ent}(D)=-\sum_{i=1}^{|\mathcal{Y}|}p_k\log_2 p_k$, 信息增益越大, 有助于分类的信息越多.
**过滤式选择**: 过滤式方法先对数据集进行特征选择, 然后再训练学习器, 特征选择过程与后续学习器无关. 这相当于先用特征选择工程队初始特征进行过滤, 再用过滤后的特征来训练模型. 常见的有 Relief 方法, 该方法设计了 "相关统计量" 来度量特征的重要性. 为每个初始特征赋予一个 "相关统计量", 度量特征的重要性. 特征子集的重要性由子集中每个特征所对应的相关统计量之和决定. 设计一个阈值, 然后选择比阈值大的相关统计量分量所对应的特征. 或者指定欲选取的特征个数, 然后选择相关统计量分量最大的指定个数特征. 通过猜中近邻, 猜错近邻与公式 $\delta^{j} = \sum_{i}-\operatorname{diff}(x_{i}^{j}, x_{i,nh}^{j})^{2}+\operatorname{diff}(x_{i}^{j}, x_{i,nm}^{j})^{2}$ 求得各属性的相关统计量分量. Relief 只需要在数据集的采样上估计相关统计量, Relief 的时间开销随采样次数以及原始特征数线性增长, 是一个运行效率很高的过滤式特征选择算法. Relief 是为了二分类问题设计的, 其变体 Relief-F 可以处理多分类问题. 假定数据集 $D$ 中的样本来自 $|\mathcal{Y}|$ 个类别. 若 $x_i$ 属于第 $k$ 类, 则先在第 $k$ 类样本中找猜中近邻 $x_{i,nh}$, 并在第 $k$ 类之外每个类中找到一个 $x_{i}$ 的最近示例作为猜错近邻, 记作 $x_{i,l,nm}$. 于是相关统计量对应于属性 $j$ 的分量为 $\delta^{j}=\sum_{i}-\operatorname{diff}(x_{i}^{j}, s_{i,nh}^{j})^{2}+\sum_{l\neq k}(p_l \times \operatorname{diff}(x_i^{j},x_{i,l,nm}^{j})^{2})$, 其中 $p_l$ 为第 $l$ 类样本所占比例.
**包裹式选择与过滤式选择区别**: 与过滤式特征选择不考虑后续学习器不同, 包裹式特征选择直接把最终将要使用的学习器性能作为特征子集的评价准则. 由于包裹式特征选择方法直接针对给定学习器进行优化, 因此从最终学习器性能来看, 包裹式特征选择比过滤式特征选择更好, 但另一方面由于包裹式特征选择过程中要多次训练学习器, 因此计算开销通常大得多.
**包裹式选择**: 拉斯维加斯方法 LVW 是一个典型的包裹式特征选择方法. 它在拉斯维加斯方法框架下使用随机策略进行子集搜索, 并以最终分类器的误差维特征子集评价准则. 若有时间限制, 则拉斯维加斯方法或给出满足要求的解, 或不给出解; 而蒙特卡洛方法一定会给出解, 但是这个解未必满足要求. 若无时间限制, 则两者都能给出满足要求的解.
**嵌入式选择与前两者区别**: 在过滤式和包裹式特征选择方法中, 特征选择过程与学习器训练过程有明显的分别; 而嵌入式特征选择是将特征选择过程与学习器训练过程荣威一起, 即在学习器训练过程中自动地进行了特征选择. **稀疏解**: 将岭回归中的 L2 换为 L1 范数则有 LASSO $\min_{\omega}\sum_{i=1}^{m}(y_i-\omega^{\mathrm{T}}x_i)^{2}+\lambda \left\| \omega \right\|_{1}$. L1 范数和 L2 范数都有助于降低过拟合风险, 但前者还会带来一个额外的好处: 它比后者更容易获得稀疏解, 即求得的 $\omega$ 会有更少的非零分量. L0 范数 (非零元素个数) 会得到更为稀疏的解, 但是 L0 范数不连续, 难以优化求解. L1 范数等值线与平方误差等值线交点更常出现在坐标轴上. **嵌入式选择**: 注意到 $\omega$ 取得稀疏解意味着初始 $d$ 个特征中仅有对应着 $\omega$ 的非零分量特征才会出现在最终模型中, 因此结果是得到了仅含一部分初始特征的模型, 换言之基于 L1 正则化的学习方法就是一种嵌入式特征选择方法. L1 正则化问题可使用近端梯度下降 PGD 来解决.
**近端梯度下降**(PGD)：$\min_{\boldsymbol{x}} \sum_{i=1}^m f(\boldsymbol{x}) + \lambda \left\| \boldsymbol{x} \right\|_{1} $ **L-Lipschitz 条件**：若 f(x) 可导，则满足 存在 L > 0：$\left\| \nabla f(\boldsymbol{x'}) - \nabla f(\boldsymbol{x}) \right\|_2^2 \leqslant L \left\| \boldsymbol{x}' - \boldsymbol{x} \right\|_{2}^2$;则在 $\boldsymbol{x}_k$ 附近的泰勒展开为：$\hat{f}(\boldsymbol{x}) \approx f(\boldsymbol{x_k}) + <\nabla f(\boldsymbol{x_k}),\boldsymbol{x} - \boldsymbol{x_k}> + \frac{L}{2} \left\| \boldsymbol{x}-\boldsymbol{x_k} \right\|^2 = \frac{L}{2} \left\| \boldsymbol{x}  - (\boldsymbol{x_k} - \frac{1}{L} \nabla f(\boldsymbol{x_k}))\right\|_{2}^2 + \text{const} $ 则当：$\boldsymbol{x_{k+1}} = \boldsymbol{x_k} - \frac{1}{L} \nabla f(\boldsymbol{x_k})$ 使得 $\hat{f}(\boldsymbol{x})$ 取得最小值，再用梯度下降法，逐步下降。类似的，问题转化为：$\boldsymbol{x}_{k+1} = \arg \min_{\boldsymbol{x}} \frac{L}{2} \left\| \boldsymbol{x}  - (\boldsymbol{x_k} - \frac{1}{L} \nabla f(\boldsymbol{x_k}))\right\|_{2}^2 + \lambda \left\| \boldsymbol{x} \right\|_{1}$,因为 各分量之间没有影响，所以假设 $\boldsymbol{z} = \boldsymbol{x_k} - \frac{1}{L} \nabla f(\boldsymbol{x_k})$, 有闭式解：$x_{k+1}^i = z^i - \frac{\lambda}{L}, \frac{\lambda}{L} < z^i ; 0, |z^i| \leqslant \frac{\lambda}{L};z^i + \frac{\lambda}{L}, z^i < -\frac{\lambda}{L} $
**字典学习**：**稀疏表示** 优势：文本数据线性可分,存储高效. 为普通稠密表达的样本找到合适的字典，将样本转化为稀疏表示，这一过程称为字典学习。$\min_{\boldsymbol{B,\alpha_i}} \sum_{i=1}^m\left\| \boldsymbol{x_i} - \boldsymbol{B} \boldsymbol{\alpha_i} \right\|_{2}^2 + \lambda \left\| \boldsymbol{\alpha_i} \right\|_{1}$ 其中 $\boldsymbol{\alpha_i}$ 为 $\boldsymbol{x_i}$ 的稀疏表示。这里需要学习 $\boldsymbol{B}$ 和 $\boldsymbol{\alpha_i}$ 。首先固定 $\boldsymbol{B}$,对 $\boldsymbol{\alpha_i}$ 进行优化，即：$\min_{\boldsymbol{\alpha_i}} \left\| \boldsymbol{x_i} - \boldsymbol{B} \boldsymbol{\alpha_i} \right\|_{2}^2 + \lambda \left\| \boldsymbol{\alpha_i} \right\|_{1} $ 然后 固定 $\boldsymbol{\alpha_i}$ 对 $\boldsymbol{B}$ 进行优化：使用 KSVD 进行优化（逐行更新策略）$\min_{\boldsymbol{B}} \left\| \boldsymbol{X} - \boldsymbol{BA}  \right\|_{F}^2 = \min_{b_i,\alpha^i} \left\| (X - \sum_{j\neq i} b_i\alpha^j)- b_i\alpha^i \right\|_{F}^2 = \min_{b_i,\alpha^i} \left\| E_i- b_i\alpha^i \right\|_{F}^2$对$E_i$ 进行奇异值分解，取得最大奇异值对应的正交向量。为了不破坏 $\alpha_i$ 的稀疏性，$\alpha_i$ 仅保留非零元素，$E_i$ 仅保留 $b_i$ 与 $\alpha_i$ 非零元素的乘积项。
**压缩感知**：能否利用部分数据恢复全部数据？长度为 m 的离散信号，采样得到 长度为 n 的信号 y,$n\ll m$, 我们希望通过某个线性变换，使得：$\boldsymbol{y} = \Phi \boldsymbol{x} = \Phi \Psi \boldsymbol{s} = \boldsymbol{As}$。$A$ 具有限行等距性，可以还原出 $s$
**限定等距性**（RIP）：对于任意子矩阵 $\boldsymbol{A_k}$ 和 $\boldsymbol{s}, \delta_k \in (0,1)$ $(1-\delta_k)\left\| \boldsymbol{s} \right\|_{2}^2 \leqslant \left\| \boldsymbol{A_k s}  \right\|_{2}^2 \leqslant (1 + \delta_k)\left\| \boldsymbol{s} \right\|_{2}^2$。若 $\boldsymbol{A}$ 满足 k 限定等距性，则有通过 $\boldsymbol{y}$ 近乎完美的恢复出 $\boldsymbol{s}$ : 在一定条件下， $L_0$ 范数 与 $L_1$ 范数同解，只需考虑：$\min_{\boldsymbol{s}} \left\| \boldsymbol{s} \right\|_{1}, s.t. \boldsymbol{y} = \boldsymbol{As} $然后将该问题转化为类似 LASSO 形式求解。重要应用：矩阵补全
**基础知识**: 令 $h$ 是从 $\mathcal{X}$ 到 $\mathcal{Y}$ 的一个映射, 其泛化误差为 $E(h;\mathcal{D})=P_{x\sim \mathcal{D}}(h(x)\neq y)$, 经验误差为 $\hat{E}(hlD)=\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(h(x_i)\neq y_i)$. 由于 $D$ 是 $D$ 的独立同分布采样, 因此 $h$ 的经验误差的期望等于其泛化误差. 令 $\epsilon$ 为 $E(h)$ 的上限, 即 $E(h)\le \epsilon$, 我们通常用 $\epsilon$ 表示预设的学得模型所应满足的误差要求, 亦称误差参数. 我们可以用不合来衡量两个映射间的差别 $d(h_1,h_2)=P_{x\sim \mathcal{D}}(h_1(x)\neq h_2(x))$. 常用的几个不等式有 Jensen 不等式, 对于任意凸函数 $f(x)$ 有 $f(\mathbb{E}(x))\le \mathbb{E}(f(x))$. Hoeffding 不等式 $P(\frac{1}{m}\sum_{i=1}^{m}x_i-\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}(x_i)\ge \epsilon)\le \exp(-2m\epsilon^{2})$.
**PAC 学习理论**: 概率近似正确学习理论. 概念 $c$ 指从样本空间 $\mathcal{X}$ 到标记空间的映射. 若对任何 $(x,y)$ 有 $c(x) = y$ 成立, 则称 $c$ 为目标概念. 我们将所有希望学到的目标概念所构成集合称为概念类 $\mathcal{C}$. 给定算法 $\mathfrak{L}$, 它所考虑的所有可能概念的集合称为假设空间 $\mathcal{H}$. 若目标概念 $c\in \mathcal{H}$, 则 $\mathcal{H}$ 中存在假设能够将所有示例按与真实标记一致的方式完全分开, 我们则称该学习算法 $\mathfrak{L}$ 是可分的, 也称一致的. 我们希望能够以较大概率学得误差满足预设上限的模型, 即概率近似正确.
PAC 辨识: 对 $0<\epsilon, \delta<1$, 所有 $c\in \mathcal{C}$ 和分布 $\mathcal{D}$, 若存在学习算法 $\mathfrak{L}$, 其输出假设 $h \in \mathcal{H}$ 满足 $P(E(h) \le \epsilon) \ge 1 - \delta$, 则称学习算法 $\mathfrak{L}$ 能从假设空间 $\mathcal{H}$ 中 PAC 辨识概念类 $\mathcal{C}$, 即 $\mathfrak{L}$ 能以较大概率 (至少为 $1-\delta$) 学得目标概念 $c$ (误差最多为 $\epsilon$). **PAC 可学习**: 令 $m$ 表示从分布 $\mathcal{D}$ 中独立同分布采样得到的样例数目, $0 < \epsilon, \delta < 1$, 对所有分布 $\mathcal{D}$, 若存在学习算法 $\mathfrak{L}$ 和多项式函数 $\operatorname{poly}(\cdot, \cdot, \cdot, \cdot)$, 使得对任何 $m \ge \operatorname{poly}(1 / \epsilon, 1 / \delta, \operatorname{size}(x), \operatorname{size}(c))$, $\mathfrak{L}$ 能从假设空间 $\mathcal{H}$ 中 PAC 辨识概念类 $\mathcal{C}$, 则称概念类 $\mathcal{C}$ 对假设空间 $\mathcal{H}$ 而言是 PAC 可学习的. **PAC 学习算法**: 若学习算法 $\mathfrak{L}$ 使概念类 $\mathcal{C}$ 为 PAC 可学习的, 且 $\mathfrak{L}$ 的运行时间也是多项式函数 $\operatorname{poly}(1 / \epsilon, 1 / \delta, \operatorname{size}(x), \operatorname{size}(c))$, 则称概念类 $\mathcal{C}$ 是高效 PAC 可学习的, 称 $\mathfrak{L}$ 为概念类 $\mathcal{C}$ 的 PAC 学习算法. **样本复杂度**: 满足 PAC 学习算法 $\mathfrak{L}$ 所需的 $m \ge \operatorname{poly}(1 / \epsilon, 1 / \delta, \operatorname{size}(x), \operatorname{size}(c))$ 中最小的 $m$, 称为学习算法 $\mathfrak{L}$ 的样本复杂度.
**有限假设空间可分情形**: 由于 $c \in \mathcal{H}$, 我们只需要将 $\mathcal{H}$ 中不满足的假设剔除. 我们先估计泛化误差大于 $\epsilon$ 但仍表现完美的概率, 假定 $h$ 泛化误差大于 $\epsilon$, 对任何样例 $(x,y)$ 有 $P(h(x)=y)=1-P(h(x)\neq y)=1-E(h)<1-\epsilon$. 表现一致概率为 $P(h(x_1)=y_1 \land \cdots)=(1-P(h(x)\neq y))^{m}<(1-\epsilon)^{m}$. 我们不知道是哪个 $h$, 我们只需要保证 $|\mathcal{H}|$ 中概率和不大于 $\delta$ 即可 $P(h\in \mathcal{H}: E(h)>\epsilon \land \hat{E}(h)=0)<|\mathcal{H}|(1-\epsilon)^{m}<|\mathcal{H}|e^{-m\epsilon}\le \delta$. 对于剩下的等效假设, 我们有 $m \ge \frac{1}{\epsilon}(\ln|\mathcal{H}|+\ln \frac{1}{\delta})$. 因此可知有限假设空间 $\mathcal{H}$ 都是 PAC 可学习的. 输出假设 $h$ 的泛化误差随样例数目的增多而收敛到 0, 收敛速率为 $O(\frac{1}{m})$. **有限假设空间不可分情形**: 当 $c \notin \mathcal{H}$ 时, 学习算法 $\mathfrak{L}$ 无法学得目标概念 $c$ 的 $\epsilon$ 近似. 但是我们可以找出 $\mathcal{H}$ 中泛化误差最小的假设 $\argmin_{h\in \mathcal{H}}E(h)$, 因此可以将目标推广到 $c \notin \mathcal{H}$ 的情况, 即不可知 PAC 学习, 满足 $P(E(h) - \min_{h'\in \mathcal{H}}E(h')\le \epsilon)\ge 1-\delta$.
**为什么需要 VC 维**: 现实学习任务所面临的通常是无限假设空间, 例如实数域中的所有区间等. 欲对此种情形的可学习性进行研究, 需要度量假设空间的复杂度, 最常见的方法就是考虑假设空间的 VC 维.
**增长函数**: 记标记结果 $h|_{D} = \{ (h(x_1), h(x_2), \cdots, h(x_m)) \}$, 则增长函数为 $\Pi_{\mathcal{H}}(m) = \max_{\{ x_1,\cdots ,x_m \}\subseteq \mathcal{X}}|\{ (h(x_1), h(x_2), \cdots, h(x_m)) | h \in \mathcal{H} \}|$. 增长函数 $\Pi_{\mathcal{H}}(m)$ 表示假设空间对 $m$ 个示例所能够赋予标记的最大可能结果数. 因此, 增长函数描述了假设空间 $\mathcal{H}$ 的表示能力, 由此反映出假设空间的复杂度.
对分和打散: $\mathcal{H}$ 中的假设对 $D$ 中示例赋予标记的每种可能结果称为对 $D$ 的一种对分. 若假设空间 $\mathcal{H}$ 能够实现 $D$ 上的所有对分, 即 $\Pi_{\mathcal{H}}(m)=2^{m}$, 则称示例集 $D$ 能被假设空间 $\mathcal{H}$ 打散. 我们可以使用增长函数来估计经验误差和泛化误差之间的关系 $P(|E(h)-\hat{E}(h)|>\epsilon)\le 4\Pi_{\mathcal{H}}(2m)\exp(-\frac{m\epsilon^{2}}{8})$.
**VC 维计算**: 假设空间 $\mathcal{H}$ 的 VC 维是能够被 $\mathcal{H}$ 打散的最大示例集大小, 即 $\operatorname{VC}(\mathcal{H})=\max\{ m: \Pi_{\mathcal{H}}(m)=2^{m} \}$. 我们通常这样计算: 若存在大小维 $d$ 的示例集能被 $\mathcal{H}$ 打散, 但不存在任何大小为 $d+1$ 的示例集能被 $\mathcal{H}$ 打散, 则 $\mathcal{H}$ 的 VC 维是 $d$. **VC 维定理**: 我们有 Sauer 引理 $\Pi_{\mathcal{H}}(m) \le \sum_{i=0}^{d}\binom{m}{i}\le (\frac{e\cdot m}{d})^{d}$, 其中 $\Pi_{\mathcal{H}}(m) \le \sum_{i=0}^{d}\binom{m}{i}\le \sum_{i=0}^{d}\binom{m}{i}(\frac{m}{d})^{d-i}\le (\frac{m}{d})^{d}\sum_{i=0}^{m}\binom{m}{i}(\frac{d}{m})^{i}=(\frac{m}{d})^{d}(1+\frac{d}{m})^{m}\le (\frac{e\cdot m}{d})^{d}$, 即可得增长函数的上界, 以及基于 VC 维的泛化误差解. 该泛化误差界只与 $m$ 有关, 收敛速率为 $O(\frac{1}{\sqrt{m}})$, 与数据分布 $\mathcal{D}$ 和样例集 $D$ 无关. 因此, 基于 VC 维的泛化误差界是分布无关, 数据独立的. 若有 $\hat{E}(h)=\min_{h'\in \mathcal{H}}\hat{E}(h')$, 则称 $\mathfrak{L}$ 维满足经验风险最小化 ERM 原则的算法. 同时我们定理: 任何 VC 维有限的假设空间 $\mathcal{H}$ 都是 (不可知) PAC 可学习的.
**Rademacher 复杂度**: 经验 Rademacher 复杂度 $\hat{R}_{D}(\mathcal{F}) = \mathbb{E}_{\sigma}[\sup_{h\in \mathcal{H}}\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}f(x_{i})]$, 其中 $\sigma_{i}$ 是 Rademacher 随机变量. 其中 $h$ 和 $x$ 换成 $f$ 和 $z$ 即可从分类任务变为回归任务. 当 $|\mathcal{H}| = 1$ 时, 结果为 $0$; 当 $|\mathcal{H}|=2^{m}$ 时, 结果为 1. Rademacher 复杂度 $R_{m}(\mathcal{H})=\mathbb{E}_{D\subseteq \mathcal{D}:|D|=m}[\hat{R}_{D}(\mathcal{F})]$.
**与 VC 维相比区别**: VC 维对任何数据分布都成立, 这使得结果具有一定的普适性; 但从另一方面来说, 由于没有考虑到数据自身, 基于 VC 维得到的泛化误差界通常比较松. Rademacher 复杂度是另一种刻画假设空间复杂度的途径, 与 VC 维不同的是, 它在一定程度上考虑了数据分布.
**Rademacher 复杂度相关定理**: 可得到关于函数空间 $\mathcal{F}$ 的泛化误差界, 其与分布 $\mathcal{D}$ 和数据 $D$ 有关. 基于 Rademacher 复杂度的泛化误差界依赖于具体学习问题上的数据分布, 类似于量身定制, 因此通常比 VC 维的泛化误差界更紧.
**损失函数**: 损失函数刻画了假设 $\mathfrak{L}_{D}$ 的预测标记 $\mathfrak{L}_{D}(x)$ 与真实标记 $y$ 之间的差别. 泛化损失 $\ell(\mathfrak{L},\mathcal{D})=\mathbb{E}_{x\in \mathcal{X},z=(xy)}[\ell(\mathfrak{L},z)]$. 经验损失 $\hat{\ell}(\mathfrak{L},D)=\frac{1}{m}\sum_{i=1}^{m}\ell(\mathfrak{L}_{D},z_i)$. 留一损失 $\ell_{loo}(\mathfrak{L},D)=\frac{1}{m}\sum_{i=1}^{m}\ell(\mathfrak{L}_{D^{\setminus i}},z_i)$. **稳定性评价什么**: 前两者推导的泛化误差界均与具体学习算法无关. 而算法的稳定性考察算法在输入发生变换时, 输出是否会随之发生较大的变化. 令 $z = (x, y)$, $\ell$ 为损失函数, 若 $|\ell(\mathfrak{L}_{D}, z) - \ell(\mathfrak{L}_{D^{\setminus i}},z)|\le \beta$, 则称 $\mathfrak{L}$ 关于 $\ell$ 满足 $\beta$-均匀稳定性. **稳定性相关的定理**: 若损失函数 $\ell$ 有界, 则经验损失和泛化误差之间差别的收敛率为 $\beta\sqrt{m}$, 令 $\beta = O(\frac{1}{m})$, 则有收敛域 $O(\frac{1}{\sqrt{m}})$, 与前两者一致. 若学习算法 $\mathfrak{L}$ 满足 ERM 且稳定, 则假设空间 $\mathcal{H}$ 可学习. 稳定性与假设空间并未无关, 两者通过损失函数 $\ell$ 联系起来.
**半监督学习**: 先用有标记样本 $D_{l}$ 训练一个模型, 再使用该模型寻找对改善模型性能帮助大的样本, 询问专家对应的标记, 这样可以使用较少的标记构建出较强的模型, 大幅降低标记成本, 这种学习方式称为**主动学习**. 虽然未标记样本未直接包含标记信息, 但是它们与有标记样本是独立同分布采样的, 其中包含的关于数据分布的信息对建立模型大有裨益. 因此, 我们让学习器不依赖外界交互, 自动地利用未标记样本来提升学习性能, 就是**半监督学习**. **聚类假设**假定数据存在簇结构, 同簇样本同属一个类别. **流形假设**假设数据分布在一个流形结构上, 相邻样本拥有过相似输出值. 流形假设可以看作聚类假设的推广, 但流形假设对输出值没有限制, 因此适用于更多类型的学习任务. 聚类假设和流形假设本质都是相似样本具有相似输出. **纯半监督学习**假定训练数据中的未标记样本并非待预测的数据, 而**直推学习**假定学习过程中所考虑的未标记样本恰是待预测数据, 学习的目的就是在这些未标记样本上获得最优泛化性能. 即纯半监督学习是基于开放世界假设, 而直推学习是基于封闭世界假设.
**半监督高斯混合模型** (生成式半监督学习): 生成式方法是直接基于生成式模型的方法, 此类方法假设所有数据都是由同一个潜在的模型生成的. 该假设使得我们能够通过潜在模型的参数将未标记数据与学习目标联系起来, 而未标记数据的标记则可看作模型的缺失参数, 通常可基于 EM 算法进行极大似然估计求解. 高斯混合模型：$p(\boldsymbol{x}) = \sum_{i=1}^k \alpha_i \cdot  p(\boldsymbol{x} | \boldsymbol{\mu_i},\boldsymbol{\Sigma_i})$ 其中：$p \boldsymbol{x} | \boldsymbol{\mu_i},\boldsymbol{\Sigma_i}) = \frac{1}{(2\pi)^{\frac{n}{2}}|\boldsymbol{\Sigma_i}|^{\frac{1}{2}}} \exp (-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu_i})^T \boldsymbol{\Sigma_i}^{-1}(\boldsymbol{x} - \boldsymbol{\mu_i}))$ 假设样本独立同分布，且由同一个高斯混合模型生成，则对数似然函数：$\ln p(D_l \cup D_u) = \sum_{(\boldsymbol{x_j},y_j)\in D_l} \ln (\sum_{i=1}^k \alpha_i \cdot p(\boldsymbol{x_j}|\boldsymbol{\mu_i},\boldsymbol{\Sigma_i}) \cdot  p(y_j|\Theta = i,\boldsymbol{x_j}))+\sum_{\boldsymbol{x_j} \in D_u} \ln (\sum_{i=1}^k \alpha_i\cdot p(\boldsymbol{x_j}|\boldsymbol{\mu_i},\boldsymbol{\Sigma_i})) $。使用EM算法求解：E 步： 根据当前模型，计算未标记样本属于各高斯混合成分的概率：$\gamma_{ji} = \frac{\alpha_i p(x_j |\mu_i,\Sigma_i)}{\sum_{i=1}^N \alpha_i p(x_j |\mu_i,\Sigma_i)}$ M 步：基于 $\gamma_{ji}$ 更新模型参数，其中 $l_i$ 表示第 i 类中有标记样本数目： $\mu_{i} = \frac{1}{\sum_{x_j \in D_u} \gamma_{ji} + l_i} (\sum_{x_j \in D_u} \gamma_{ji} x_j + \sum_{(x_i,y_i)\in D_l \wedge y_j=i} x_j)$; $\Sigma_i = \frac{1}{\sum_{x_j \in D_u} \gamma_{ji} + l_i} (\sum_{x_j \in D_u} \gamma_{ji} (x_j - \mu_i)(x_j-\mu_i)^T + \sum_{(x_i,y_i)\in D_l \wedge y_j=i}(x_j - \mu_i)(x_j-\mu_i)^T)$,$\alpha_i = \frac{1}{m} (\sum_{x_j \in D_u} \gamma_{ji} + l_i)$假设：所有样本（有无标记），都是由混合高斯模型生成的。用极大似然估计来处理半监督样本。此类算法简单, 易于实现, 在有标记数据极少时效果往往比其他方法好. 然而, 此类算法模型假设必须准确, 即生成式模型必须与真实数据分布吻合, 否则使用未标记数据反而会显著降低泛化性能.
**半监督 SVM** (基于伪标记的半监督学习): 半监督支持向量机 S3VM 在考虑未标记样本后, 试图找到能将两类有标记样本分开, 且穿过数据低密度区的划分超平面, 这里的基本假设是 "低密度分隔", 这是聚类假设在考虑了线性超平面划分后的推广. 缺点是计算效率低, 可能存在多个低密度分隔线. 其中著名的有 TSVM, 其试图对未标记样本进行各种可能的标记指派, 尝试将每个未标记样本分别作为正例或反例, 然后寻求一个间隔最大化的划分超平面. 基本流程为, 首先在有标记数据上训练一个 SVM: $\min \frac{1}{2} \left\| w \right\|_{2}^2 + C_l \sum_{i=1}^l \epsilon_i + C_u \sum_{i=1}^m \epsilon_i; s.t. y_i(w^Tx_i + b) \geqslant 1-\epsilon_i, \hat{y}_i(w^Tx_i + b) \geqslant 1-\epsilon_i, \epsilon_i \geqslant 0$, 然后利用这个 SVM 的预测结果作为 "伪标记" 赋予未标记样本, 加入到训练集一同训练, 但是会赋予较小的权重 $C_{u}$, 因为可能会打错误标记, 然后找出两个标记指派为异类且很可能会发生错误的未标记样本, 交换它们的标记, 再重新进行训练, 且逐渐增大 $C_{u}$, 直到 $C_{u}=C_{l}$ 为止. 同时考虑到伪标签可能会造成类别不平衡，所以将 $C_u$ 修改为 $C_u^+ = \frac{u_-}{u_+} C_u^-$。伪标签算法, 选取最确信的样本, 即距离超平面最远的优先打标签. 伪标签算法和标记传播算法和具体分类回归任务无关, 皆可以适用.
**图半监督学习** (基于标记传播的半监督学习): 将数据集映射为图, 每个样本对应图中的一个结点, 若两个样本相似度很高, 则它们之间存在一条边, 边的强度正比于样本之间的相似度. 我们认为有标记样本结点染过色, 未标记样本结点未染色, 于是半监督学习就对应与颜色在图上扩散或传播的过程. 我们可以使用矩阵运算进行图的分析和推导. 常见的有标记传播方法. 给定一个数据集, 我们可将其映射为一个图, 数据集中每个样本对应于图中一个结点, 若两个样本之间的相似度很高(或相关性很强), 则对应的结点之间存在一条边, 边的“强度”(strength)正比于样本之间的相似度(或相关性)。我们可将有标记样本所对应的结点想象为染过色, 而未标记样本所对应的结点则尚未染色. 于是, 半监督学习就对应于“颜色”在图上扩散或传播的过程。定义一个图：G = (V,E),$V = \{ x_1\cdots x_l,\cdots x_{l+u} \}$
边集 E 表示为一个亲和矩阵，常基于高斯函数定义：$\sigma>0$ 为用户指定的高斯函数带宽参数。 $W_{ij} =   \exp (\frac{-\| x_i-x_j \|_2^2}{2\sigma^2}), i\neq j；0,otherwise$ 假定从 G 上学得了一个实值函数 $f :V\to \mathbb{R}$ : $E(f) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m W_{ij}(f(x_i) - f(x_j))^2 = \frac{1}{2}(\sum_{i=1}^m d_i f^2(x_i) + \sum_{j=1}^m d_j f^2(x_j) - 2\sum_{i=1}^m \sum_{j=1}^m W_{ij}f(x_i)f(x_j) ) = \boldsymbol{f}^T(\boldsymbol{D}-\boldsymbol{W})\boldsymbol{f}$ 其中 $\boldsymbol{f} = (f_l^T,f_u^T)^T, D = diag(d_1,d_2\cdots d_{l+u})$, $d_i = \sum_{j=1}^{l+u}(W_{ij})$ 为矩阵$W$ 的第i行元素。具有最小能量的函数 $f$ 在有标记样本上满足 $f(\boldsymbol{x_i}) = y_i$,在未标记样本上满足：$ \boldsymbol{\Delta f} = 0 $,其中 $\boldsymbol{\Delta} = \boldsymbol{D-W}$ 接着采用分块矩阵表示：$W = [W_{ll}, W_{lu}; W_{ul},W_{uu}]$ ，则 $E(f) = f_{l}^T (D_{ll} - W_{ll})f_l - 2f_u^T W_{ul}f_l + f^T_u(D_{uu} - W_{uu})f_u$。由 $\frac{\mathrm{d}E(f)}{\mathrm{d}f_u} = 0$ 可得 $f_u = (D_{uu} - W_{uu})^{-1}W_{ul}f_l$。综上可对未标记信息 $f_u$ 进行预测。假设为 能量函数假设，使用有标签样本作为输入，通过让 能量函数最小化求得一个 未标记样本得预测标签 $\boldsymbol{f}_u$。优点：图半监督学习方法在概念上相当清晰, 且易于通过对所涉矩阵运算的分析来探索算法性质。缺点（1）存储开销高（2）由于构图过程仅考虑训练样本集, 难以判知新样本在图中的位置, 因此, 在接收到新样本时, 或是将其加入原数据集对图进行重构并重新进行标记传播, 或是需引入额外的预测机制。
**协同学习** (基于分歧的半监督学习): 与前几者不同, 基于分歧的方法使用多学习器, 学习器之间的分歧对未标记数据的利用至关重要. 协同训练是此类方法的重要代表, 它最初是针对多视图数据设计的, 也被看作多视图学习的代表. 一个数据对象往往拥有多个属性集, 每个属性集就构成了一个视图. 不同视图的相容性指其所包含的关于输出空间 $\mathcal{Y}$ 的信息是一致的. 互补性指关于输出空间的信息可以互相补充. 协同训练很好地利用了多视图的相容互补性. 假设数据拥有两个充分且条件独立视图, 充分指每个视图都包含注意产生最优学习器的信息, 条件独立指在给定类别标记条件下两个视图独立. 因此利用未标记数据方式为, 每个视图上基于有标记样本训练出一个分类器, 并对最有把握的未标记样本赋予伪标记, 提供给另一个分类器作为新增有标记样本训练. 为了减少在所有样本上计算分类置信度的开销, 可以使用未标记样本缓冲池. 理论指出, 若两个视图充分且条件独立, 可利用未标记样本通过协同训练将弱分类器的泛化性能提升到任意高. 即使在更弱的条件下, 协同训练仍可有效地提升弱分类器性能. 即使不用多视图, 只需要不同弱学习器之间存在显著分歧时, 均可提升泛化性能.
自训练缺陷: 有标记样本较少时, 学习出来的分类器性能会很弱, 那么后来赋予无标记样本的伪标记中错误就会较多, 最后重复训练的学习器会有很大程度上是在拟合数据噪声.
**半监督聚类** (两种监督信息和对应的 K-means 改进): 第一种是必连和勿连约束, 前者指样本必属于一个簇, 后者指样本必不属于一个簇. 第二种是少量地有标记样本. 约束 k 均值算法是利用第一类监督信息的代表, 改动为在将 $x_i$ 划入簇时会判断是否违背约束, 由近及远, 直到找到一个簇, 找不到则报错. 第二种监督学习可以直接将它们作为种子, 初始化 $k$ 个聚类中心, 并且在迭代过程中不改变簇隶属关系, 即为约束种子 k 均值.
**概率模型**: 概率模型提供了一种描述框架, 将描述任务归结为计算变量的概率分布, 在概率模型中, 利用已知的变量推测未知变量的分布称为推断, 其核心在于基于可观测的变量推测出未知变量的条件分布. 关心变量集合为 $Y$, 可观测变量集合为 $O$, 其他变量集合为 $R$. 生成式模型考虑联合分布 $P(Y, R, O)$, 判别式模型考虑条件分布 $P(Y, R | O)$. 推断即为得到条件概率分布 $P(Y | O)$. 为什么要用图概率模型：直接利用概率求和规则消去变量R的时间和空间复杂度为$O(2^{|Y|+|R|})$指数级别 ，需要一种能够简洁紧凑表达变量间关系的工具。分类：有向图（贝叶斯网）；无向图（马尔可夫网）
**生成式模型**基于贝叶斯定理可写为 $P(c|x)=\frac{P(c)P(x|c)}{P(x)}$, 其中 $P(c|x)$ 称为后验概率, $P(c)$ 称为先验概率, $P(x|c)$ 是样本 $x$ 相对于类标记 $c$ 的条件概率, 或称为似然, $P(x)$ 是用于归一化的证据因子. 估计类条件概率的一种策略是先假定其具有某种确定的概率分布形式, 再基于训练样本对概率分布的参数进行估计. 对于类条件概率 $P(x|c)$, 其被 $\theta_c$ 唯一确定, 则我们需要利用训练集 $D$ 估计参数 $\theta_c$, 我们将其记作 $P(x|\theta_c)$. 我们有 $P(D_c|\theta_c) = \prod_{x\in D_c}P(x|\theta_c)$, 对 $\theta_c$ 进行极大似然估计即为 $\hat{\theta}_c=\argmax_{\theta_c}LL(\theta_c)$, 其中 $LL(\theta_c)=\sum_{x\in D_c}\log P(x|\theta_c)$ 称为对数似然函数. 对于有向无环图 DAG 的贝叶斯网, 若在给定 $x_3$ 时 $x_1, x_2$ 才独立, 称为条件独立 (同父结构); 若在不给定 $x_3$ 时 $x_1, x_2$ 才独立, 称为边际独立 (V 型结构). 为了分析有向图中变量的条件独立性, 我们使用有向分离方法, 将有向图转为一个无向图: 找出有向图中所有的 V 型结构, 在 V 型结构的两个父节点之间加上一条无向边; 将所有有向边改为无向边. 由此产生的图称为道德图, 令父相连的过程称为道德化.
**隐马尔科夫模型**: 隐马尔科夫模型 HMM 是结构最简单的动态贝叶斯网, 这是一种著名的有向图模型, 主要用于时序数据建模. 由状态变量 (隐变量) $y$ 和观测变量 (显变量) $x$ 组成, 任一时刻观测变量的取值仅依赖于状态变量, 当前时刻状态仅依赖于前一时刻变量. 模型由状态转移概率, 输出观测概率和初始状态概率组成, 即 $\lambda=[A,B,\pi]$. 其中, $a_{ij}=P(y_{t+1}=s_j|y_t=s_i)$, $b_{ij}=P(x_t=o_j|y_t=s_i)$, $\pi_i=P(y_1=s_i)$. 联合概率密度分布为 $P(x_1,y_1,\cdots,x_n,y_n)=P(y_1)P(x_1|y_1)\prod_{i=2}^{n}P(y_i|y_{i-1})P(x_i|y_i)$. 第一个问题为给定 $\lambda$ 如何计算 $x$ 的 $P(x|\lambda)$, 即评估模型和观测序列的匹配程度; 第二个问题为给定 $\lambda$ 和 $x$ 如何找到最佳 $y$, 即根据观测序列推出隐藏的模型状态; 第三个问题为给定 $x$ 如何调整 $\lambda$ 使得 $P(x|\lambda)$ 最大.
**马尔可夫随机场**: 马尔可夫随机场 MRF 是典型的马尔可夫网, 是一种著名的无向图模型. 马尔可夫随机场有一组势函数, 亦称因子, 这是定义在变量子集上的非负实函数, 主要用于定义概率分布函数. 图中的一个子集, 任意两结点都有边连接, 则称为团; 若加入另外任意一个结点都不成团, 则称为极大团. 基于极大团构造势函数 $\psi_{Q}$, 联合概率分布可以使用极大团集合 $\mathcal{C}^{*}$ 定义 $P(x) = \frac{1}{Z^{*}}\prod_{Q\in \mathcal{C}^{*}}\psi_{Q}(x_{Q})$, 其中 $Z^{*} = \sum_{x}\prod_{Q\in \mathcal{C}^{*}}\psi_{Q}(x_{Q})$ 为规范因子. 为了得到条件独立性, 我们使用概念分离, 即若 $A$ 中结点到 $B$ 中结点都必须经过 $C$ 中结点, 则称 $A$ 和 $B$ 被 $C$ 分离, $C$ 为分离集. 全局马尔可夫性指, 给定两个变量子集的分离集, 则这两个变量子集条件独立, 记作 $x_{A} \perp x_{B} | x_{C}$, 即 $P(x_{A}, x_{B}|x_{C}) = P(x_{A}|x_{C})P(x_{B}|x_{C})$. 有两个推论, 局部马尔可夫性, 给定某变量的邻接变量, 则该变量条件独立于其他变量, 即令 $n(v)$ 为 $v$ 的邻接结点, $n^{*}(v)=n(v)\cup \{ v \}$, 则有 $x_{v} \perp x_{V\setminus n^{*}(v)}|x_{n(v)}$; 成对马尔可夫性, 给定所有其他变量, 两个非邻接变量条件独立, 即若 $\left<u, v\right>\notin E$, 则 $x_{u}\perp x_{v}|x_{V\setminus\left< u,v\right>}$. 势函数 $\psi_{Q}(x_{Q})$ 作用是定量刻画变量集 $x_{Q}$ 中变量的相关关系, 应为非负函数, 且在所偏好的变量取值上有较大的函数值. 为了满足非负性, 常常使用指数函数 $\psi_{Q}(x_{Q})=e^{-H_{Q}(x_{Q})}$, $H_{Q}(x_{Q})$ 是一个实值函数, 常见为 $H_{Q}(x_{Q})=\sum_{u\neq v}\alpha_{uv}x_{u}x_{v}+\sum_{v\in Q} \beta_{v}x_{v}$.
**条件随机场**: 条件随机场 CRF 是一种判别式无向图模型. 与之不同, 前面两个模型均为生成式模型. 条件随机场试图对多个变量在给定观测值后的条件概率进行建模, 目标是构建条件概率模型 $P(y|x)$. 标记变量 $y$ 可以是结构型变量, 即分量之间具有某种相关性. 结点与标记变量 $y$ 中元素一一对应的无向图, 若每个变量 $y_{v}$ 都满足马尔可夫性 $P(y_{v}|x,y_{V\setminus \{ v \}}) = P(y_{v}|x,y_{n(n)})$, 则 $(y, x)$ 构成一个条件随机场. 最常用的是链式条件随机场, 包含两种关于标记变量的团, 相邻的标记变量 $\{ y_{i-1},y_{i} \}$ 和单个标记变量 $y_{i}$, 并使用势函数和团结构来定义条件概率 $P(y|x)$, 且还需要定义合适的状态特征函数和转移特征函数. 条件概率被定义为：$P(\boldsymbol{y}|\boldsymbol{x}) = \frac{1}{Z} \exp (\sum_{j}\sum_{i=1}^{n-1} \lambda_j t_j (y_{i+1},y_i,\boldsymbol{x},i) + \sum_{k}\sum_{i=1}^n \mu_k s_k (y_i,\boldsymbol{x},i) )$ (1)$t_j(y_{i+1},y_i,\boldsymbol{x},i)$是定义在观测序列的两个相邻标记位置上的转移特征函数，用于刻画相邻标记变量之间的相关关系以及观测序列对它们的影响 (2)$s_k(y_i,\boldsymbol{x},i)$是定义在观测序列的标记位置 上的状态特征函数，用于刻画观测序列对标记变量的影响 (3)$\lambda_j,\mu_k$为参数， Z为规范化因子。CRF 和 MRF 均使用团上的势函数定义概率; 但 CRF 处理的是条件概率, 而 MRF 处理的是联合概率.
**学习和推断**推断：基于概率图模型定义的分布，能对目标变量的边际分布或某些可观测变量为条件的条件分布进行推断。学习：对概率图模型，还需确定具体分布的参数，称为参数估计或学习问题，通常使用极大似然估计或后验概率估计求解。推断问题的目标就是计算边际概率 $P(x_{F})$ 或者条件概率 $P(x_{F}|x_{E})$, 最后可以归结于计算边际分布 $P(x_{E}) = \sum_{x_{F}}P(x_{E},x_{F})$. 推断方法分为两类：
**精确推断**：实际上是动态规划法，一般情况下，该类方法的计算复杂度随极大团规模增长呈指数增长，适用范围有限。变量消去是最直观的精确推断方法得到, 也是构建其它精确推断算法的基础, 该方法对无向图同样适用, 但是如果要计算多个边际分布, 则会造成冗余计算. 信念传播算法中, 一个结点仅在接收到来自其它所有结点的消息后才能向另一个结点发送消息, 且结点的边际分布正比于它所接收的消息的乘积。 若图中没有环, 从所有叶结点开始向根节点传递消息，直到根节点收到所有邻接结点的消息；然后从根结点开始向叶结点传递消息得到, 直到所有叶结点均收到消息.
**近似推断(采样法)**:（例如MCMC）：通过随机化方法完成近似。很多任务中，我们关心的并非概率分布本身，而是基于概率分布的期望，并且还能基于期望进一步作出决策。若直接计算或逼近这个期望比推断概率分布更容易，则直接操作无疑将使推断问题更为高效。根据 $p(x)$ 抽取样本计算 $\hat{f}=\frac{1}{N}\sum f(x_{i})$ 即可逼近 $\mathbb{E}_{p}[f]=\int f(x)p(x)\mathrm{d}x$. 最常用的是马尔科夫链蒙特卡洛 MCMC 方法. MCMC先构造出服从分布的独立同分布随机变量, 再得到无偏估计. MCMC 算法的关键在于通过构造平稳分布为 $p$ 的马尔可夫链来产生样本, 当马尔可夫链运行足够长的时间 (收敛到平稳状态), 则产出的样本 $x$ 近似服从 $p$ 分布. 若在某个时刻满足马尔可夫链平稳条件 $p(x^{t})T(x^{t-1}|x^{t}) = p(x^{t-1})T(x^{t}|T(x^{t}|x^{t-1}))$, 则 $p(x)$ 是该马尔可夫链的平稳分布, 且马尔可夫链在满足该条件时已经收敛到平稳状态. MCMC中如何构造马尔可夫链的转移概率至关重要，不同构造方法产生不同的MCMC算法。MH算法：拒绝接受采样. 算法每次根据上一轮采样结果 $x_{t-1}$ 来采样获得状态样本 $x^*$,但这个样本会被一定概率拒绝。若从状态 $x^{t-1}$ 到状态 $x^*$ 的转移概率为 $Q(x^*|x^{t-1})A(x^*|x^{t-1})$,$x^*$ 最终收敛到平衡状态，则：$p(\boldsymbol{x}^{t-1})Q(\boldsymbol{x}^* |\boldsymbol{x}^{t-1})A(\boldsymbol{x}^*|\boldsymbol{x}^{t-1}) = p(\boldsymbol{x}^{*})Q(\boldsymbol{x}^{t-1} |\boldsymbol{x}^{*})A(\boldsymbol{x}^{t-1}|\boldsymbol{x}^{*})$ $A(x^*|x^{t-1}) = \min(1, \frac{p(\boldsymbol{x}^{*})Q(\boldsymbol{x}^{t-1} |\boldsymbol{x}^{*})}{p(\boldsymbol{x}^{t-1})Q(\boldsymbol{x}^* |\boldsymbol{x}^{t-1})})$
吉布斯采样被视为 MH 算法的特例，也是用马尔可夫链获取样本，平稳分布也是 $p(x)$。具体来说，假定文本为 $\boldsymbol{x_1},\boldsymbol{x_2}\cdots \boldsymbol{x_N}$, 目标分布为 $p(\boldsymbol{x})$, 在初始化$\boldsymbol{x}$的取值后, 通过循环下列步骤来完成采样：(1)随机或以某个次序选取变量 $x_i$ (2) 根据 $\boldsymbol{x}$ 中除 $x_i$ 外的变量的现有取值，计算条件概率 $p(x_i|\boldsymbol{x_i})$,其中 $\boldsymbol{x_i} = \{ x_1,\cdots x_{i-1}, x_{i+1},\cdots x_N \}$(3) 根据 $p(x_i|\boldsymbol{x_i})$ 对变量 $x_i$ 采样，用采样值代替原值。
**变分推断：使用确定性近似完成推断**变分推断通过使用已知简单分布来逼近需推断的复杂分布, 并通过限制近似分布的类型, 从而得到一种局部最优, 但具有确定解的近似后验分布. 其图只有一个 $z$ 作为父节点, 和互相独立的子节点 $x_i$, 联合分布概率密度函数为 $p(x|\Theta)=\prod_{i=1}^{N}\sum_{z}p(x_i,z|\Theta)$. 对应的记法叫做盘式记法（相互独立的、由相同机制生成的多个变量被放在一个方框（盘）内，并在方框中标出类似变量重复出现的个数N、通常用阴影标注出已知的、能观察到的变量）使用 EM 算法, 在 E 步, 根据 $t$ 时刻的参数 $\Theta^{t}$ 对 $p(z|x,\Theta^{t})$ 进行推断, 计算联合似然函数 $p(x,z|\Theta)$; 在 M 步, 基于 E 步的结果进行最大化寻优, 即对关于变量 $\Theta$ 的函数 $\mathcal{Q}(\Theta; \Theta^{t})$ 进行最大化从而求取 $\Theta^{t+1}=\argmax_{\Theta}\mathcal{Q}(\Theta;\Theta^{t})=\argmax_{\Theta}\sum_{z}p(z|x,\Theta^{t})\ln p(x,z|\Theta)$, $\mathcal{Q}(\Theta; \Theta^{t})$ 实际上是对数联合似然函数 $\ln p(x,z|\Theta)$ 在分布 $p(z|x,\Theta^{t})$ 下的期望, 当分布 $p(z|x,\Theta^{t})$ 与变量 $z$ 的真实后验分布相等时, $\mathcal{Q}(\Theta;\Theta^{t})$ 近似等于对数似然函数.
**话题模型**：话题模型（topic model）是一类生成式有向图模型，主要用来处理离散型的数据集合（如文本集合）。作为一种非监督产生式模型，话题模型能够有效利用海量数据发现文档集合中隐含的语义。隐狄里克雷分配模型（Latent Dirichlet Allocation, LDA）是话题模型的典型代表。词（word）:待处理中的基本离散单元文档（document）：待处理数据对象，由词组成，词在文档中不计顺序。数据对象只要能用“词袋”表示就可以使用话题模型。话题（topic）：表示一个概念，具体表示为一系列相关的词，以及它们在该概念下出现的频率。数据集一共有 K 个话题和 T 篇文档，文档中的词来自一个包含 N 个词的字典。用 T 个 N 维向量 $W = \{ w_1,w_2,\cdots w_T \}$ 表示文档集合，其中 $w_{t,n}$ 表示文档 t 中词 n 的词频；用 K 个 N 维向量 $\beta_{k}$ 表示话题，$\beta_{k,n}$ 表示话题 k 中词 n 的词频用 $\Theta_t$ 表示文档 t 中包含的每个话题的比例，$\Theta_{t,k}$ 表示文档 t 中包含话题 k 的比例。
**生成文档 t**：从以$\alpha$ 为参数的狄利克雷分布中随机采样一个话题分布 $\Theta_t$;再产生文档中的N个词：（1）根据$\Theta_t$ 进行话题指派，得到文档 t 中词 n 的话题 $z_{t,n}$;（2）根据指派的话题所对应的词分布 $\beta_{k}$ 随机采样生成词。词频$w_{t,n}$ 是唯一已观测变量，依赖于话题指派 $z_{t,n}$ 及话题对应词频 $\beta_k$, 话题指派 $z_{t,n}$ 依赖于话题分布 $\Theta_t$, $\Theta_t$ 依赖于 $\alpha$, 话题词频依赖于参数 $\eta$。$p(W,z,\beta,\Theta|\alpha,\eta) = \Pi_{t=1}^T p(\Theta_t | \alpha) \Pi^k p(\beta_k|\eta) = \Pi^T_{t=1}p(\Theta_t|\alpha)\Pi^K_{i=1}p(\beta_k|\eta)(\Pi_{n=1}^N P(w_{t,n}| z_{t,n},\beta_K)P(z_{t,n}|\Theta_t))$, 模型参数估计：$LL (\alpha,\eta) = \sum_{t=1}^T \ln p(w_t|\alpha,\eta)$, 模型推断：$p(z,\beta,\Theta| W,\alpha,\eta) = \frac{p(W,z,\beta,\Theta| \alpha,\eta)}{W|\alpha,\eta}$
**规则学习**: 规则学习是从训练数据中学习出一组能够用于对未见示例进行判别的规则. 一条规则形如 $\oplus \leftarrow f_1 \land f_2 \land \cdots \land f_{L}$. 蕴含符号右边部分为规则体, 表示规则的前提, 左边部分为规则头, 表示规则的结果. 规则体是由逻辑文字 $f_{k}$ 组成的合取式, 每个文字都是对示例属性进行检验的布尔表达式. 当同一个示例被判别结果不同的多条规则覆盖时, 称发生了冲突, 解决冲突的办法称为冲突消解. 常见的冲突消解策略有投票法, 排序法和元规则法等. (1)**投票法**时将判别结果相同的规则数最多的结果作为最终结果. (2)**排序法**是在规则集合上定义一个顺序, 在发生冲突时使用排序最前的规则, 相应的规则学习过程称为带序规则学习或优先级规则学习. (3)**元规则法**是根据领域知识事先设定一些元规则, 例如使用长度最小的规则. 规则学习常常设置一条默认规则, 用来处理规则集合未覆盖的样本. 从形式语言表达能力而言, 规则可分为命题规则和一阶规则. 命题规则由原子命题和逻辑连接词构成简单陈述句; 一阶规则基本成分是能描述事物属性或关系的原子公式, 例如表达父子关系的为此. 一阶规则能表达复杂的关系, 也被称为关系型规则.
**序贯覆盖**: 规则学习的目标是产生一个能覆盖尽可能多的样例的规则集, 最直接的做法是序贯覆盖, 即逐条归纳, 在训练集上每学到一条规则, 就将该规则覆盖的训练样例去除, 然后以剩下的训练样例组成的训练集上重复上述过程. 由于每次只处理一部分数据, 因此也被称为**分治策略**. 最简单的方法是从空规则开始, 将正例类别作为规则头, 再逐个遍历训练集的每个属性及取值, 但是会由于组合爆炸而不可行. 我们可以使用自顶向下或自底向上的方式. 第一种策略为(1)**自顶向下**, 即从比较一般的规则开始, 逐渐添加新文字以缩小规则覆盖范围, 直到满足预定条件为止, 亦称生成测试法, 是规则逐渐特化的过程. 第二种策略是(2)**自底向上**, 即从比较特殊的规则开始, 逐渐删除文字以扩大规则覆盖范围, 直到满足条件为止, 亦称数据驱动法, 是规则逐渐泛化的过程. 前者通常更容易产生泛化性能较好的规则, 而后者则更适合于训练样本较少的情形, 此外, 前者对噪声的鲁棒性比后者要强得多. 因此, 在命题规则学习中通常使用第一种策略, 而第二种策略在一阶规则学习这类假设空间非常复杂的任务上使用较多. 规则评判, 增加/删除哪一个候选文字, 准确率, 信息熵增益 (率), 基尼系数. 规避局部最优, 集束搜索, 每轮保留最优的b个逻辑文字，在下一轮均用于构建候选集，再把候选集中最优的b个留待再下一轮使用.
**剪枝优化**: 规则生成本质上是一个贪心搜索过程, 需有一定的机制来缓解过拟合的风险, 最常见的做法是剪枝. 剪枝发生在规则生长过程中, 叫做**预剪枝**; 剪枝发生在规则产生后, 叫做**后剪枝**. 预剪枝可以使用 **CN2 算法**, 其假设用规则集进行预测必须显著优于直接基于训练样例集后验概率分布进行预测. CN2 使用了似然率统计量 $LBS=2\cdot (\hat{m}_{+}\log_2\frac{\hat{m}_{+}/(\hat{m}_{+}+\hat{m}_{-})}{m_{+}/(m_{+}+m_{-})}+\hat{m}_{-}\log_2\frac{\hat{m}_{-}/(\hat{m}_{+}+\hat{m}_{-})}{m_{-}/(m_{+}+m_{-})})$, 其中 $m_{+}$, $m_{-}$ 分别表示训练样例集中的正, 反例数目, $\hat{m}_{+}$, $\hat{m}_{-}$ 分别表示规则集覆盖的正, 反例数目. LBS 越大, 说明采用规则集进行预测于直接使用训练集正例反例比率进行猜测的差别越大; LBS 越小, 说明规则集越可能是偶然现象. 通常设置 LRS 很大时才停止生长. **后剪枝**最常用的策略是**减错剪枝 REP**, 其基本做法是, 将样例集划分为训练集和验证集, 在训练集上学得规则集 $\mathcal{R}$ 后, 穷举所有可能的剪枝操作 (删除文字, 删除规则), 用验证集反复剪枝直到准确率无法提高. REP 剪枝复杂度较高, 为 $O(m^{4})$, 我们可以使用 IREP 将其降到 $O(m\log^{2} m)$. **IREP**：在生成每条规则前，先将当前样例集划分为训练集和验证集，在训练集和验证集，在训练集上生成一条规则 $r$, 立即在验证集上对其进行 REP 剪枝，得到规则 $r'$,将 $r'$ 覆盖的样例去除，然后继续。IREP* 用 $\frac{\hat{m}_+ + (m_- - \hat{m}_-)}{m_++m_-}$ 取代了以准去率作为规则性能衡量标准；在剪枝时删除规则尾部多个文字；并在得到规则集后再进行一次IREP。**RIPPER**：$r_i'$: 基于 $r_i$ 覆盖的样例，用 IREP* 重新生成规则，称为“替换规则”。$r_i''$: 对 $r_i$ 增加规则，然后用 IREP* 剪枝生成一条规则，修订规则。进行比较留下来更好的哪一个。RIPPER 的优势是将所有规则放在一起优化，通过全局的考虑来缓解序贯覆盖的局部性.
**一阶规则学习**: 受限于命题逻辑表达能力, 命题规则学习难以处理对象之间的关系, 而关系信息在很多任务中非常重要. 且一阶规则学习能容易地引入领域知识. **FOIL** 是著名的一阶规则学习算法, 它遵循序贯覆盖框架且采用自顶向下的规则归纳策略, FOIL 在规则生成时需考虑不同的变量组合, 其使用 FOIL 增益来选择文字. $F_{Gain} = \hat{m}_+ \times (\log_2 \frac{\hat{m}_+}{\hat{m}_+ + \hat{m}_-} - \log_2 \frac{m_+}{m_+ + m_-})$. 仅以正例数作为权重，是因为关系数据正例数往往远小于反例数。FOIL 可大致看作命题规则学习于归纳逻辑程序设计之间的过渡, 其自顶向下的规则生成过程不能支持函数和逻辑表达式嵌套, 因此规则表达能力仍有不足; 但它把命题规则学习过程通过变量替换等操作直接转化为一阶规则学习, 因此比一般归纳逻辑程序设计技术更高效.
**归纳逻辑程序设计**: 归纳逻辑程序设计 ILP 目标为完备地学习一阶规则 (Horn 子句), 仍然以序贯覆盖方法学习规则集. 一般采用自底向上策略学习单条规则, 不需要列举所有可能的候选规则, 对目标概念的搜索维持在样例附近的局部区域, 自顶向下策略的搜索空间对于规则长度呈指数级增长. 为什么用自底向上的方式：因为函数和逻辑表达式的嵌套问题通过自顶向下无法列举所有候选文字。最小一般泛化 LGG 中, 泛化指将覆盖率低的规则变换为覆盖率高的规则, 一般指覆盖率尽可能高, 最小指变换时对原规则的改动尽可能小. LGG 的步骤：（1）找出两条规则中涉及相同谓词的文字.（2）考察谓词后括号里的项：若文字相同，则保持不变$lgg(t,t) = t,$如果不同则用同一个新变量代替 $ lgg(s,t) = V, s\neq t$.
**强化学习**: 强化学习任务使用马尔可夫决策过程 MDP 来描述, 机器处在环境 $E$ 中, 状态空间: $X$. 动作空间: $A$. 转移函数 $P: X\times A \times X \mapsto \mathbb{R}$. 奖赏函数 $R: X \times A \times X$ 或 $R: X \times X$ (仅与状态转移有关). 强化学习任务对应四元组 $E = \left<X, A, P, R \right>$. 机器要做的就是学一个策略 $\pi$, 进而得知在状态 $x$ 下要执行的动作 $a = \pi(x)$. 确定性策略 $\pi: X \mapsto A$; 随机性策略 $\pi: X\times A \mapsto \mathbb{R}$, $\pi(x,a)$ 为状态 $x$ 下选择动作 $a$ 的概率, 其中 $\sum_{a}\pi(x,a)=1$. 策略的优劣取决于长期执行这一策略后得到的累积奖赏, 长期累计奖赏常用的计算方式有 T 步累计奖赏 $\mathbb{E}[\frac{1}{T}\sum_{t=1}^{T}r_{t}]$ 和 $\gamma$ 折扣累计奖赏 $\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r_{t+1}]$.
**多摇臂赌博机**: 最大化单步奖赏, 即仅考虑一步操作. 单步强化学习任务对应了 K-摇臂赌博机模型. **仅探索法**将所有机会平分给每个摇臂, 最后每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计; **仅利用法**按下目前最优的摇臂. 仅探索法失去了很多选择最优摇臂的机会, 而仅利用法没有很好地估计摇臂期望奖赏, 很可能经常选不到最优摇臂. 探索和利用两者是矛盾的, 因为尝试次数有限, 加强一方自然会削弱一方, 这就是强化学习面临的探索-利用窘境. 欲使累积奖赏最大, 则必须在探索和利用之间达到较好的折中. **解决方法**:（1）**$\epsilon$-贪心**：每次尝试时，以 $\epsilon$ 的概率进行探索。用 $Q(k)$ 记录平均奖赏：$Q(k) = \frac{1}{n} \sum_{i=1}^n v_i$（$v_i$ 为第 $i$ 次使用k摇臂获得的奖赏. 用更新的方式去计算 $Q(k)$: $Q_n(k) = \frac{1}{n}((n-1)Q_{n-1}(k) + v_n)= Q_{n-1}(k) + \frac{1}{n}(v_n - Q_{n-1}(k))$. 在一段时间后可以很好近似, 不再需要探索, 可以取值 $\epsilon= 1 / \sqrt{t}$. (2) **Softmax**: 若某些摇臂的平均奖赏明显高于其他摇臂，则它们被选取的概率也明显更高。基于 Boltzmann 分布：$P(k) = \frac{\exp(Q(k)/\tau)}{\sum_{i=1}^K \exp(Q(i)/\tau)}$.$\tau$ 被称为温度，$\tau$ 越小，平均奖赏高的摇臂被选取的概率越高。对于多步强化学习, 一种直接办法是将每个状态上动作的选择看作一个 K-摇臂赌博机问题, 用强化学习的累积奖赏来代替 K-摇臂赌博机算法中的奖励函数, 即可将赌博机算法用于每个状态.
**有模型强化学习**: 多步强化学习任务中马尔可夫决策过程四元组已知, 这种情形称为模型已知, 对应的学习称为有模型学习. 令状态值函数 $V^{\pi}(x)$ 表示从状态 $x$ 出发, 使用策略 $\pi$ 所带来的累积奖赏; 状态-动作值函数 $Q^{\pi}(x,a)$ 表示从状态 $x$ 出发, 执行动作 $a$ 后再使用策略 $\pi$ 带来的累积奖赏. **累计奖赏**：$V (b) = E[\frac{1}{T}\sum_{t=1}^T r_t | x_{0} = b], V(b) = E[\sum_{t=0}^\infty \gamma^t r_{t+1} | x_{0} = b]$. **状态动作值函数**：$Q (b,a) = E[\frac{1}{T}\sum_{t=1}^T r_t | x_{0} = b, a_0 = a], Q(b,a) = E[\sum_{t=0}^\infty \gamma^t r_{t+1} | x_{0} = b, a_0 = a]$. **综上有**：$V_T(x) =  E_\pi [\frac{1}{T} \sum_{t=1}^T r_t|x_0 = x]= \sum_{a \in A} \pi (x,a) \sum_{x' \in X}P^a_{x\to x'}(\frac{1}{T} R^a_{x\to x'} + \frac{T-1}{T} V^\pi_{t-1}(x')), V_\gamma(x) = \sum_{a \in A} \pi (x,a) \sum_{x' \in X}P^a_{x\to x'}(R^a_{x\to x'} +\gamma V_\gamma(x'))$. 有了状态值函数 V, 就能计算出**状态-动作值函数**：$Q_T(x,a) =  \sum_{x' \in X}P^a_{x\to x'}(\frac{1}{T} R^a_{x\to x'} + \frac{T-1}{T} V^\pi_{t-1}(x')), Q_\gamma =  \sum_{x' \in X}P^a_{x\to x'}(R^a_{x\to x'} +\gamma V_\gamma(x'))$. 由于 $P$ 和 $R$ 已知, 值函数有很简单的递归形式, 按照递归等式来计算值函数, 实际上就是一种动态规划算法. **最优 Bellman 等式**: 理想的策略 $\pi^* = \argmax_\pi \sum_{x \in X}V^\pi (x), V^*(x) = \max_{a \in  A} Q^{\pi^*}(x,a)$. 可得最优状态-值函数为：$Q^*_T(x,a) =  \sum_{x' \in X } P^a_{x\to x'} (\frac{1}{T}R^a_{x\to x'} + \frac{T-1}{T} \max_{a' \in A} Q^*_\gamma(x',a')), Q^*_\gamma(x,a) =  \sum_{x' \in X } P^a_{x\to x'} (R^a_{x\to x'} + \gamma \max_{a' \in A} Q^*_\gamma(x',a'))$. 值函数对于策略的每一点改进都是单调递增的，因此对于当前策略 $\pi $, 可以放心地将其改进为：$\pi'(x) = \argmax_{a \in A} Q_\pi(x,a)$. 策略迭代指从一个初始策略出发, 不断迭代地进行策略评估和改进, 直到策略收敛, 不再改变为止. **策略迭代**：(1)进行 T-step 计算出 T 轮后的累计值(其中 $\pi(x,a) = \frac{1}{|A(x)|}$). (2)计算 $\pi'(x) = \argmax_{a \in A} Q(x,a)$. (3)如果 $\forall x,\pi '(x) \neq \pi (x)$ ，返回 (1)(4)输出 $\pi (x)$. 策略迭代算法在每次改进策略后都需要重新进行策略评估, 这通常比较耗时. 我们知道策略改进与值函数改进是一致地, 因此可将策略改进视为值函数地改善, 得到值迭代算法, 其最后输出策略. **值迭代**：(1)T-step 用$V'(x) = max_{a \in A} \sum_{x' \in X} P^a_{x\to x'}(\frac{1}{t}R + \frac{t-1}{t} V(x'))$, 替代原先的值计算方式. (2) 输出 $\pi (x) = \argmax_{a \in  A}Q(x,a)$.
**免模型强化学习**: 在现实的强化学习任务中, 环境的转移概率, 奖赏函数往往难以获知, 若学习算法不依赖于环境建模, 则称为免模型学习. **蒙特卡洛强化学习**: 从起始状态出发, 使用**某种策略 $\pi$** 进行采样, 执行该策略 T 步并获得轨迹 $\left<x_0,a_0,r_1,x_1,a_1,r_2,\cdots,x_{T-1},a_{T-1},r_{T},x_{T} \right>$, 对轨迹中出现的每一对状态-动作, 记录其后的奖赏之和, 作为该状态-动作对的一次累积奖赏采样值, 多次采样后进行平均, 得到状态动作值函数的估计. 我们在原始策略上使用 $\epsilon$-贪心法, 因此最优动作被选中的概率是 $1-\epsilon+\frac{\epsilon}{|A|}$. 因为 $\pi^{\epsilon}$ 仅是将 $\epsilon$ 概率均匀分配给所有动作, 因此对最大化值函数的原始策略 $\pi'$, 同样有 $Q^{\pi}(x,\pi'(x))\ge V^{\pi}(x)$, 可以使用同样的方法进行策略改进, 由于被评估与被改进的是同一个策略, 因此称为同策略 on-policy 蒙特卡罗强化学习算法. 同策略蒙特卡罗算法最终产生的是 $\epsilon$-贪心策略, 但是我们不需要在使用策略时进行 $\epsilon$-贪心, 因此我们有在策略评估时 $\epsilon$-贪心, 在策略改进时改进原始策略的异策略 off-policy 蒙特卡罗强化学习算法. 由于蒙特卡罗算法没有充分利用强化学习任务的 MDP 结构, 需要完成一个采样轨迹后才进行更新策略的值估计, 效率低得多.  时序差分 TD 学习则结合了动态规划和蒙特卡罗方法的思想, 能做到更高效的免模型学习. 蒙特卡罗其中求平均来作为期望累积奖赏的近似, 在采样了一个完整的轨迹才进行求平均更新. 实际上这个更新过程能增量式完成, 这就是时序差分算法. TD 的增量求和更改为：$Q^\pi _{t+1}(x,a) = Q^\pi _t(x,a) + \alpha(R^a_{x\to x'} + \gamma Q^\pi_t(x',a') - Q^\pi _t(x,a) )$. 这里，将 $\alpha$ 类似于 $\frac{1}{t+1}$ 的比例系数，$R_{x\to x'}^a + \gamma Q(x',a')$ 看作单步奖励和累计奖励之和。其中 SARSA 为同策略 TD 算法, Q-learning 为异策略 TD 算法.
**值函数近似**: 有限状态空间上的值函数是关于有限状态的表格值函数, 即值函数能表示为一个数组, 改变一个状态的值不会影响其他状态的值. 而实际的强化学习任务所面临的状态空间往往是连续的, 有无穷多的状态. 即使想要将状态空间离散化, 离散化的方式也是一个困难. 实际上可以直接对连续状态空间的值函数进行学习, 最简单的即是状态的线性函数 $V_{\theta}(x)=\theta^{\mathrm{T}}x$, 由于难以精确记录每个状态的值, 因此这样值函数的求解被称为值函数近似.
**模仿学习**: 在现实任务中, 往往能得到人类专家的决策过程范例, 从这样的范例中学习, 称为模仿学习. 强化学习任务中多步决策的搜索空间巨大, 基于累积奖赏来学习很多步之前的合适决策非常困难, 而直接模仿人类专家的状态-动作对可限制缓解这一困难, 我们称其为直接模仿学习. 使用监督学习学得符合人类专家决策的初始策略, 再使用强化学习方法进行学习. 在很多任务中, 设计奖赏函数往往相当困难, 从人类专家提供的范例数据中反推出奖赏函数有助于解决该问题, 这就是逆强化学习.
