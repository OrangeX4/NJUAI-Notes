# 矩阵和向量应用实例

1. 欧式距离, 马氏距离, T 距离
2. 聚类, 分类, K-NN, 度量学习
3. 人脸识别应用

## 聚类和分类的相似比较

- 聚类
    - hard clustering: 每个样本只能属于一个簇
        - k-means (k 均值聚类)
    - soft clustering: 一个样本可以被归为多个簇
- 分类



### k-means (k 均值聚类)

- 交替优化 (Alternating Opt): 每次固定一个变量, 解另一个变量
- 基本上可以证明一定会收敛: 因为每一步都一定会单调递减
- k-means 就是优化目标函数 E, 也被称作均方误差和 (Sum of Squared Error, **SSE**), 具体证明可以看 [PS5 第六题](../机器学习/Homework/PS5/PS5.md)

有以下关键:

1. k 如何选取
2. 初始样本如何选定
    1. 随机初始化可能很差
    2. 随机选几个, 然后选取 SSE 最小的


### k 近邻学习器

能否准确地找到 k 近邻?

**密采样假设**: 样本的每个 $\epsilon$-邻域都有近邻.

如果阈值设为 $10^{-3}$, 假设维度为 $20$, 则密采样需要的样本数接近 $10^{60}$.

注意, 就连一张不是很清晰的图像都有 70 余万维.


### 流形学习 - ISOMAP

- 关键思路: 测地线距离 (近似), 保距
- 步骤
    - 构造近邻图
    - 基于最短路径算法近似任意两点之间的测地线距离
    - 基于距离矩阵通过 MDS 获得低维嵌入

为什么可以用近邻图: 如果两个样本很像, 则不管怎么变换还是压缩, 依旧很像

1. 高维近邻 == 逼近 ==> 低维近邻
2. 欧式近邻 ==> 真实近邻

ISOMAP 是一种流形学习的方法, 用于非线性数据降维. 它的基本步骤如下: 

1. 构造近邻图: 对于每个点, 选择最近的 $K$ 个点作为邻居, 并将每个点和它的邻居相连, 我们将两个点之间的相似度作为边的长度. 
2. 计算所有点之间的测地距离: 利用 Dijkstra 算法或者 Floyd 算法, 求出任意两个点之间在邻居图上的最短路径长度, 作为它们之间的测地距离. 
3. 降维映射: 利用 MDS 算法, 根据测地距离矩阵, 将高维数据映射到低维空间中. 


## 距离测度

两个向量之间的距离 $D(p||q)$ 满足

- 正定性: 任意两个向量之间的距离都是非负的, 且当且仅当两个向量相等时, 距离为零. 
- 齐次性: 任意两个向量之间的距离与这两个向量的长度成正比. 
- 三角不等式: 任意三个向量之间, 任意两个向量之间的距离不大于另外两对向量之间的距离之和. 

距离是相异度的度量.

### 距离度量的种类

- 欧氏距离: 很少见的, 但是在近邻问题中总是有效的, 可以用来接近任何距离度量. 因为任意 **连续** 距离度量 $D$ 依据其泰勒展开一定有 $D(x, s)|_{x \to s} \approx |x - s| + \left\| x - s \right\|^{2}$.
- 曼哈顿距离
- 测地距离

### 距离度量学习

希望找到一个 "合适" 低维空间下的距离度量. (能否直接 "学出" 合适的距离?)

要能通过 "参数化" 来学习距离度量.

马氏距离是一个很好的选择.

$$
\operatorname{dist}_{\mathrm{mah}}^{2}(x_i, x_j) = (x_i-x_j)^{\mathrm{T}}M(x_i-x_j)
$$

其中 $M$ 称为 "度量矩阵", 数学上它是一个半正定对称矩阵 (一般是协方差矩阵). 距离度量学习就是要对 $M$ 进行学习

**为什么是马氏距离?**

欧式距离的缺陷: 各个方向同等重要.

**M 怎么学习?**

要有一个合适的目标函数.

- 目标 1: 结合具体分类器的性能
    - 例如, 以近邻分类器的性能为目标, 则得到经典的 NCA 算法.
- 目标 2: 结合领域知识
    - 例如, 若已知 "必连" 约束集合 $\mathcal{M}$ 和 "勿连" 约束集合 $\mathcal{C}$, 则可以通过求解下述凸优化问题得到 $M$.
  
$$
\min_{M} \sum_{(x_i, y_j) \in \mathcal{M}} \left\| x_i - x_j \right\|_{M}^{2}  \\
s.t. \sum_{(x_i, y_j) \in \mathcal{C}} \left\| x_i - x_j \right\|_{M}^{2} \ge 1  \\
M \succeq 0
$$

### 余弦度量

余弦相似度 (Cosine Similarity). 两个向量之间夹角的余弦值, 公式为: 

$$
D(x, s_i) = \cos(\theta_i) = \frac{x^{\mathrm{T}}s_i}{\left\| x \right\|_{2}\left\| s_i \right\|_{2}}
$$

也是一个相异度的有效测度.

经常用在新闻分类上

### Jaccard 距离与应用

Tanimoto 系数 (Jaccard 系数).一种用于度量两个集合之间相似度的指标, 它定义为两个集合的交集大小除以它们的并集大小. 

$$ T(A,B) = \frac{|A \cap B|}{|A \cup B|} $$

其中 $A$ 和 $B$ 是两个集合, $|A|$ 表示集合 $A$ 的元素个数, $\cap$ 表示交集, $\cup$ 表示并集. 


## 人脸识别

- 稀疏表示
- 特征脸



## 特殊矩阵

- 对称矩阵
- 对角矩阵, 上三角矩阵, 下三角矩阵
- 正定矩阵


### 对称矩阵

1. $\forall A, A + A^{\mathrm{T}}, A A^{\mathrm{T}}, A^{\mathrm{T}}A$ 是对称矩阵.
2. $A$ 对称矩阵, 则 $A^{k}$ 是对称矩阵
3. $A$ 对称矩阵且非奇异, $A^{-1}$ 是对称矩阵
4. $A, B$ 对称, $\forall \alpha, \beta$ 有 $\alpha A + \beta B$ 对称
5. $A, B$ 对称, 则 $AB$ 不一定对称, 但 $AB + BA$ 一定对称.
    1. $AB = \begin{bmatrix} \alpha_1 \beta_1 & \alpha_1 \beta_2 \\ \alpha_2 \beta_1 & \alpha_2 \beta_2 \\\end{bmatrix}$
    2. $AB + BA = \begin{bmatrix} 2\alpha_1 \beta_1 & \alpha_1 \beta_2 + \alpha_2 \beta_1 \\ \alpha_1 \beta_2 + \alpha_2 \beta_1 & 2\alpha_2 \beta_2 \\\end{bmatrix}$


### Hermite 矩阵

Hermite 矩阵的正定性: 一个 $n \times n$ Hermite 矩阵 $A$ 是正定的, 当且仅当它满足以下任意一个条件:

1. 二次型函数 $x^{\mathrm{H}}Ax > 0, \forall x \neq 0$
2. 矩阵 $A$ 的所有特征值大于零
3. 存在一个非奇异 $n \times n$ 矩阵 $R$, 使得 $A = R^{\mathrm{H}}R$
    1. 对称矩阵可 SVD 分解 $A = B^{\mathrm{T}}\Lambda B$, 其中 $B$ 是正交矩阵, $\Lambda$ 是对角阵. 因此我们有 $\sqrt{\Lambda} B$
    2. 有了 SVD 分解之后, 求逆也十分简单
4. 对任意非奇异的 $n \times n$ 矩阵 $P$, 使得 $P^{\mathrm{H}}AP$ 是正定阵.

最简单 Hermite 矩阵莫过于单位阵.


### 置换矩阵与互换矩阵

1. 用置换矩阵 $P_4$ 右乘矩阵 $A$, 相当于对 $A$ 列重排.
2. 用置换矩阵 $P_5$ 左乘矩阵 $A$, 相当于对 $A$ 行重排.
3. 置换矩阵是正交矩阵: $P^{\mathrm{T}}P = PP^{\mathrm{T}} = I$, 也就是 $\P^{-1} = P^{\mathrm{T}}$.

斜对角线全为 $1$ 的置换矩阵, 又称反射矩阵.


### 广义置换矩阵与选择矩阵

一个正方矩阵称为广义置换矩阵, 简称 $g$-矩阵, 若每行每列有且仅有一个非零元素. 一个广义置换矩阵可以分解为一个置换矩阵和一个非奇异的对角矩阵的乘积, 即有 $G = PD$, 其中 $D$ 为非奇异对角矩阵.

顾名思义, 选择矩阵是一种可以对某个给定矩阵的某些行或某些列进行选择的矩阵. 以 $m \times  N$ 矩阵为例.

$J_1 = [I_{m-1}, 0_{m-1}]$

$J_2 = [0_{m-1}, I_{m-1}]$

是两个 $(m-1) \times m$ 矩阵, 式中 $I_{m-1}, 0_{m-1}$ 分别是 $(m-1) \times (m-1)$ 单位矩阵和 $(m-1) \times 1$ 零向量, 直接计算得.

$J_1 X$ 是将 $X$ 的前 $m - 1$ 行选出来, 剩下一行置 0.

$XJ_1$ 是将 $X$ 的后 $m - 1$ 行选出来, 剩下一行置 0.

剩下同理.

**应用案例**:

非零因子 $\beta_j$, 同时源信号 $s_j(t)$ 乘以一个相同的因子, 观测数据向量也保持不变.
