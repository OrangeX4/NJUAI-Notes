{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2088,"status":"ok","timestamp":1684744944284,"user":{"displayName":"Orzrange Fang","userId":"00885665627046930540"},"user_tz":-480},"id":"AgnysjpbP_vg","outputId":"41d33f78-4037-457c-d199-5c0e607acdb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# 一些全局配置\n","config = {\n","    'is_train': True,  # 是否进行训练\n","    'is_save': True,  # 是否保存模型文件\n","    'is_load': True,  # 是否加载模型文件\n","    'is_save_result': True,  # 是否保存结果\n","\n","    # 路径相关配置\n","    'cwd': '.',  # 工作路径\n","    'model_path': '/models/bert',  # 模型保存路径\n","    'result_data': '/data/bert-result-',  # 结果保存路径\n","    'train_data': '/data/train_preprocessed.json',  # 训练数据\n","    'test_data': '/data/test_preprocessed.json',  # 测试数据\n","    'answer_data': '/data/answer.txt',\n","\n","    # 训练数据划分相关配置\n","    'random_seed': 42,  # 随机种子\n","    'train_set_ratio': 0.99,  # 训练集占训练数据的比重\n","\n","    # Model 相关配置\n","    'max_len': 120,\n","    'train_batch_size': 8,\n","    'valid_batch_size': 4,\n","    'test_batch_size': 4,\n","    \n","    # 训练相关配置\n","    'lr': 2e-05,\n","    'epochs': 6,\n","}\n","\n","# 当前是否是 google colab 中\n","is_colab = True\n","try:\n","    from google.colab import drive\n","except ImportError:\n","    is_colab = False\n","# 挂载 google drive\n","if is_colab:\n","    drive.mount('/content/drive')\n","\n","# 为 Colab 更改对应配置\n","if is_colab:\n","    config = {\n","        **config,\n","        'is_train': True,\n","        'is_save': True,\n","        'is_load': False,\n","        'cwd': '/content/drive/MyDrive/Colab Notebooks/nlp-target-sentiment-analysis',\n","    }\n","\n","# 为 Colab 安装对应依赖\n","if 'is_init' not in locals().keys() and is_colab:\n","    %pip install -q transformers\n","is_init = True"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":339,"status":"ok","timestamp":1684744944619,"user":{"displayName":"Orzrange Fang","userId":"00885665627046930540"},"user_tz":-480},"id":"WNWUeOZbP_vl"},"outputs":[],"source":["from transformers import BertModel, BertTokenizer\n","import torch\n","from torch.utils.data import Dataset\n","import torch.nn as nn\n","import json\n","\n","config['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def setup_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","    #  torch.backends.cudnn.deterministic = True\n","\n","# 设置随机数种子\n","setup_seed(config['random_seed'])\n","\n","\n","def pad(l, n, pad=0):\n","    \"\"\"\n","    Pad the list 'l' to have size 'n' using 'padding_element'\n","    \"\"\"\n","    pad_with = max(0, n - len(l))\n","    return l + pad_with * [pad]\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, data_path, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.documents = []\n","        self.targets = []\n","        self.labels = []\n","        self.dists = []\n","        self.weights = []\n","        self.max_len = max_len\n","        with open(data_path, 'r') as f:\n","            data = json.load(f)\n","            for item in data:\n","                self.documents.append(item['document'])\n","                self.targets.append(item['target'])\n","                self.labels.append(item['label'])\n","                self.dists.append(item['dist'])\n","                self.weights.append(item['weights'])\n","\n","    def __len__(self):\n","        return len(self.documents)\n","\n","    def __getitem__(self, index):\n","        document = self.documents[index]\n","        target = self.targets[index]\n","        label = self.labels[index]\n","        # document = \" \".join(document.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            document,\n","            f\"What is the sentiment class of {target} in the sentence ?\",\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        dists = pad(self.dists[index], self.max_len, 0)\n","        weights = pad(self.weights[index], self.max_len, 0)\n","        # 给 [CLS] 设置一个更大的权重 2.\n","        weights[0] = 2.\n","\n","        # get the position of the target token with tokenizer\n","        # target_ids = tokenizer.encode(target, add_special_tokens=False)\n","        # get the position of the target token with ids\n","        # target_idx = ids.index(target_ids[0])\n","        # 设置 weights 为 [0, 1, 1, 0, 0, 0, 0, 0, 0, 0]，即只关注 target, 以及 len(target_ids)\n","        # weights = [1.] + [0.] * (target_idx - 1) + [1.] * len(target_ids) + [0.] * (self.max_len - target_idx - len(target_ids))\n","\n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            # 'target_idx': torch.tensor(target_idx, dtype=torch.long),\n","            'weights': torch.tensor(weights, dtype=torch.float),\n","            'label': torch.tensor(int(label) + 1 if label != '' else -1, dtype=torch.float),\n","        }\n","\n","dataset = CustomDataset(config['cwd'] + config['train_data'], tokenizer, config['max_len'])\n","# 划分训练集和验证集\n","train_size = int(config['train_set_ratio'] * len(dataset))\n","valid_size = len(dataset) - train_size\n","train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n","# 构建 DataLoader\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=config['valid_batch_size'], shuffle=True)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5882,"status":"ok","timestamp":1684744950498,"user":{"displayName":"Orzrange Fang","userId":"00885665627046930540"},"user_tz":-480},"id":"2eCGUEelP_vn","outputId":"8eb71693-d898-4fa2-b783-c9360899a2a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["class TDBert(nn.Module):\n","\n","    def __init__(self, num_labels=3):\n","        super(TDBert, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.classifier = nn.Linear(768, num_labels)\n","\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, weights=None):\n","        last_hidden_state, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=False)\n","        # 用权重加权平均, 对 shape 为 (batch_size, seq_len, hidden_size) 的 last_hidden_state 进行加权平均\n","        # weights 的 shape 为 (batch_size, seq_len)\n","        weighted_output = torch.sum(last_hidden_state * weights.unsqueeze(-1), dim=1) / torch.sum(weights, dim=1).unsqueeze(-1)\n","        logits = self.classifier(weighted_output)\n","        return logits\n","\n","model = TDBert().to(config['device'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OpTWW5yZP_vp","outputId":"5476fbb2-b785-407b-ba20-1fef4c96e11e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Train Loss: 0.16021403670310974, Train Acc: 0.7474775910377502\n"]}],"source":["test_dataset = CustomDataset(\n","    config['cwd'] + config['test_data'], tokenizer, config['max_len'])\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=config['test_batch_size'], shuffle=False)\n","\n","\n","def predict(model, test_loader, config, is_load=False):\n","    model = model.to(config['device'])\n","    if is_load:\n","        model.load_state_dict(torch.load(\n","            config['cwd'] + config['model_path'] + '/model.pth'))\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(test_loader):\n","            input_ids = batch['input_ids'].to(config['device'])\n","            token_type_ids = batch['token_type_ids'].to(config['device'])\n","            attention_mask = batch['attention_mask'].to(config['device'])\n","            weights = batch['weights'].to(config['device'])\n","            # logits = model(input_ids, token_type_ids, attention_mask, weights)\n","            logits = model(input_ids=input_ids, attention_mask=attention_mask,\n","                           token_type_ids=token_type_ids, weights=weights)\n","            predictions.extend(torch.argmax(logits, dim=1).tolist())\n","    return predictions\n","\n","\n","def evaluate(model, valid_loader, config):\n","    criterion = nn.CrossEntropyLoss()\n","    model.eval()\n","    total_loss = 0\n","    acc = 0\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(valid_loader):\n","            input_ids = batch['input_ids'].to(config['device'])\n","            token_type_ids = batch['token_type_ids'].to(config['device'])\n","            attention_mask = batch['attention_mask'].to(config['device'])\n","            weights = batch['weights'].to(config['device'])\n","            label = batch['label'].to(config['device'])\n","            logits = model(input_ids=input_ids, attention_mask=attention_mask,\n","                           token_type_ids=token_type_ids, weights=weights)\n","            loss = criterion(logits, label.long())\n","            acc += (logits.argmax(dim=-1) == label).float().mean()\n","            total_loss += loss.item()\n","    acc /= len(valid_loader)\n","    return total_loss / len(valid_loader), acc\n","\n","\n","def train(model, train_loader, valid_loader, config):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n","    # optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n","    criterion = nn.CrossEntropyLoss()\n","    best_valid_acc = 0\n","    for epoch in range(config['epochs']):\n","        model.train()\n","        acc = 0\n","        for batch_idx, batch in enumerate(train_loader):\n","            input_ids = batch['input_ids'].to(config['device'])\n","            token_type_ids = batch['token_type_ids'].to(config['device'])\n","            attention_mask = batch['attention_mask'].to(config['device'])\n","            weights = batch['weights'].to(config['device'])\n","            label = batch['label'].to(config['device'])\n","            label = label.reshape(-1)\n","            # logits = model(input_ids, token_type_ids, attention_mask, weights)\n","            logits = model(input_ids=input_ids, attention_mask=attention_mask,\n","                           token_type_ids=token_type_ids, weights=weights)\n","            loss = criterion(logits, label.long())\n","            acc += (logits.argmax(dim=-1) == label).float().mean()\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        acc /= len(train_loader)\n","        print(\n","            f'Epoch: {epoch}, Train Loss: {loss.item()}, Train Acc: {acc.item()}')\n","        valid_loss, valid_acc = evaluate(model, valid_loader, config)\n","        print(f'valid Loss: {valid_loss}, Valid Acc: {valid_acc}')\n","        if valid_acc > best_valid_acc:\n","            best_valid_acc = valid_acc\n","            torch.save(model.state_dict(),\n","                       config['cwd'] + config['model_path'] + '/model.pth')\n","            print(f'epoch {epoch}: Save model successfully')\n","\n","\n","if config['is_train']:\n","    train(model, train_loader, valid_loader, config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DeGXTwYJP_vq"},"outputs":[],"source":["if config['is_save_result']:\n","    predictions = predict(model, test_loader, config, is_load=True)\n","    with open(config['cwd'] + config['result_data'] + '.txt', 'w') as f:\n","        for pred in predictions:\n","            f.write(str(pred) + '\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"d2l","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
